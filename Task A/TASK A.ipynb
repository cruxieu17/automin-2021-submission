{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bart-large-xsum-samsum.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fCrO6YctzGdx",
        "xJh02iTH2tGS"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8T_Vr4G76dv"
      },
      "source": [
        "# THE OFFICIAL COLAB NOTEBOOK OF TEAM ABC FOR AUTOMIN @ INTERSPEECH 2021 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ukVZnIADqw8"
      },
      "source": [
        "# MOUNT DRIVE, SET CUDA DEVICE, NECESSARY INSTALLATIONS ...\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name())\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "\n",
        "model_checkpoint = \"facebook/bart-large-xsum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "summarizer1 = pipeline(\"summarization\", model=\"/content/drive/MyDrive/AutoMin-2021/bart_large_xsum_samsum/checkpoint\", , device=0)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtGxt0BIkGsx"
      },
      "source": [
        "### CUSTOMIZED STRIP, REPLACE AND PREPROCESS FUNCTIONS ###\n",
        "\n",
        "def stripp(string):\n",
        "    list1=[]\n",
        "    string = string.strip()\n",
        "    list1[:0]=string\n",
        "    idx = 0\n",
        "    cnd = False\n",
        "    for i in list1:\n",
        "        if i.isalpha():\n",
        "            cnd = True\n",
        "            break\n",
        "    if cnd:\n",
        "        while list1[0].isalpha() == False:\n",
        "            if idx+1 == len(string):\n",
        "                break\n",
        "            list1.remove(list1[0])\n",
        "            idx+=1\n",
        "        list1 = ''.join(list1)\n",
        "    else:\n",
        "        list1 = None\n",
        "\n",
        "    return list1\n",
        "\n",
        "def preprocess(ctx):\n",
        "\n",
        "  ctx = ctx.replace(\" '\", \"'\")\n",
        "  ctx = ctx.replace(\" ,\", \",\")\n",
        "  ctx = ctx.replace(\" .\", \".\")\n",
        "  ctx = ctx.replace(\" ?\", \"?\")\n",
        "  ctx = ctx.replace(\"Ehmm\", \"\")\n",
        "  ctx = ctx.replace(\" Ehm\", \"\")\n",
        "  ctx = ctx.replace(\" mmm\", \"\")\n",
        "  ctx = ctx.replace(\" hmm\", \"\")\n",
        "  ctx = ctx.replace(\" uh\", \"\")\n",
        "  ctx = ctx.replace(\" uh ,\", \"\")\n",
        "  ctx = ctx.replace(\" uh .\", \"\")\n",
        "  ctx = ctx.replace(\" um\", \"\")\n",
        "  ctx = ctx.replace(\" um ,\", \"\")\n",
        "  ctx = ctx.replace(\" um .\", \"\")\n",
        "  ctx = ctx.replace(\" Uh\", \"\")\n",
        "  ctx = ctx.replace(\" Uh ,\", \"\")\n",
        "  ctx = ctx.replace(\" Uh .\", \"\")\n",
        "  ctx = ctx.replace(\" Um\", \"\")\n",
        "  ctx = ctx.replace(\" Um ,\", \"\")\n",
        "  ctx = ctx.replace(\" Um .\", \"\")\n",
        "  ctx = ctx.replace(\"Uh\", \"\")\n",
        "  ctx = ctx.replace(\"Um\", \"\")\n",
        "  ctx = ctx.replace(\"Yeah\", \"\")\n",
        "  ctx = ctx.replace(\" yeah\", \"\")\n",
        "  ctx = ctx.replace(\"Ehm, \", \"\")\n",
        "  ctx = ctx.replace(\"Hmm, \", \"\")\n",
        "  ctx = ctx.replace(\"Ehm. \", \"\")\n",
        "  ctx = ctx.replace(\"Hmm. \", \"\")\n",
        "  ctx = ctx.replace(\"Yeah\", \"\")\n",
        "  ctx = ctx.replace(\" yeah\", \"\")\n",
        "  ctx = ctx.replace(\"Ehm\", \"\")\n",
        "  ctx = ctx.replace(\"Hmm\", \"\")\n",
        "  ctx = ctx.replace(\"Ehm\", \"\")\n",
        "  ctx = ctx.replace(\"Hmm\", \"\")\n",
        "  ctx = ctx.replace(\"Mhm\", \"\")\n",
        "  ctx = ctx.replace(\" {disfmarker}\", \"\")\n",
        "  ctx = ctx.replace(\" {vocalsound}\", \"\")\n",
        "  ctx = ctx.replace(\" {gap}\", \"\")\n",
        "  ctx = ctx.replace(\"...\", \".\")\n",
        "  ctx = ctx.replace(\"..\", \".\")\n",
        "  ctx = ctx.replace(\",,\", \",\")\n",
        "  ctx = ctx.replace(\",,\", \",\")\n",
        "  ctx = ctx.replace(\",.\", \"\")\n",
        "  ctx = ctx.replace(\".,\", \".\")\n",
        "  ctx = ctx.replace(\"  \", \" \")\n",
        "  ctx = ctx.replace(\"(\", \"\")\n",
        "  ctx = ctx.replace(\")\", \"\")\n",
        "  ctx = ctx.replace(\"Person\", \"PERSON\")\n",
        "  ctx = ctx.replace(\"is going to\", \"will\")\n",
        "  ctx = ctx.replace(\"are going to\", \"will\")\n",
        "  ctx = ctx.replace(\"are discussing\", \"discussed\")\n",
        "  ctx = ctx.replace(\"discuss\", \"discussed\")\n",
        "  ctx = ctx.replace(\"are working\", \"worked\")\n",
        "  ctx = ctx.replace(\"is working\", \"worked\")\n",
        "\n",
        "  return ctx\n",
        "\n",
        "def replacee(i):\n",
        "  i = i.replace(\"do n't\", \"do not\")\n",
        "  i = i.replace(\"n't\", \"not\")\n",
        "  i = i.replace(\"it 's\", \"it is\")\n",
        "  i = i.replace(\" 's\", \"\")\n",
        "  if i[0]+i[1] == \"'s\":\n",
        "    i = i.replace(\"'s \", \"\")\n",
        "  i = i.replace(\"wo n't\", \"won't\")\n",
        "  i = i.replace(\" and\", \",\")\n",
        "  i = i.replace(\",,\", \",\")\n",
        "  return i\n",
        "\n",
        "### USEFUL UTIL FUNCTIONS FOR GENERATION AND FORMATTING ###\n",
        "\n",
        "def summarize(tsc):\n",
        "  a1 = summarizer1(tsc)[0]['summary_text']\n",
        "  return a1\n",
        "\n",
        "def gen_tscs(transcript_id, length):\n",
        "  tscs_preprocessed = {}\n",
        "  attendees = []\n",
        "  for k,v in t.items():\n",
        "    key = k\n",
        "    if key == transcript_id:      #IF ALL THE TRANSCIRPTS ARE NEEDED, CHANGE THIS CONDITION TO if True:\n",
        "      roles = v['roles']\n",
        "      attendees.append(list(set(roles)))\n",
        "      utterances = v['utterances']\n",
        "      tsc = ['']\n",
        "      i=0\n",
        "      for role, utterance in zip(roles, utterances):\n",
        "        utterance = preprocess(utterance)\n",
        "        v = utterance\n",
        "        v = re.sub(r\"[^a-zA-Z0-9]+\", ' ', utterance)\n",
        "        v = v.split(' ')\n",
        "        if len(v)<=4:\n",
        "          continue\n",
        "        if len(v)>4 and len(v)<7 and 's' in v:\n",
        "          continue\n",
        "        utterance = stripp(utterance)\n",
        "        if utterance == None:\n",
        "          continue\n",
        "        if len(utterance) == 1:\n",
        "          continue\n",
        "        line = role + ': ' + utterance + '\\n'\n",
        "\n",
        "        # IF DIALOGUE IS LONGER THAN \"length\"\n",
        "        tokenized_line = tokenizer.encode(line)\n",
        "        if len(tokenized_line)>=length:\n",
        "            line_ = line.split('.')\n",
        "            split_ = len(line_)//2\n",
        "            line1 = '. '.join(line_[0:split_]) + '.\\n'\n",
        "            line2 = role + ': ' + '. '.join(line_[split_:])\n",
        "            tokenized_line = [line1, line2]\n",
        "            for l in tokenized_line:\n",
        "                tokenized = tokenizer.encode(tsc[i]+l)\n",
        "                if len(tokenized)>=length:\n",
        "                    i+=1\n",
        "                    tsc.append('')\n",
        "                    tsc[i]+=l\n",
        "                else:\n",
        "                    tsc[i]+=l               \n",
        "        else:\n",
        "            tokenized = tokenizer.encode(tsc[i]+line)\n",
        "            if len(tokenized)>=length:\n",
        "                i+=1\n",
        "                tsc.append('')\n",
        "                tsc[i]+=line\n",
        "            else:\n",
        "                tsc[i]+=line\n",
        "      tscs = {key:tsc}\n",
        "      tscs_preprocessed.update(tscs)\n",
        "\n",
        "  return tscs_preprocessed, attendees\n",
        "\n",
        "def format_summary(s2):\n",
        "\n",
        "  s3 = ''.join(s2) #s2[0]\n",
        "\n",
        "  s3 = s3.split('.')\n",
        "  summ = ['']\n",
        "  id=0\n",
        "  summ1 = []\n",
        "  for i in s3:\n",
        "    #stripping the spaces\n",
        "    i = i.replace('  ', ' ')\n",
        "    if len(i) == 1:\n",
        "      continue\n",
        "    if i[0]==' ' and i[1].isalpha():\n",
        "      i = stripp(i)\n",
        "    if type(i) == type(None):\n",
        "      continue\n",
        "    if i[0] == ' ':\n",
        "      continue\n",
        "    i = preprocess(i)\n",
        "    check = re.sub(r\"[^a-zA-Z0-9]+\", ' ', i)\n",
        "    check = ''.join(i for i in check if not i.isdigit())\n",
        "    check = check.replace('  ', ' ')\n",
        "    check = check.split(' ')\n",
        "    if len(check)<=6:\n",
        "      continue\n",
        "\n",
        "    #formatting\n",
        "    if i[0] == 'P' and i[1] == 'E':\n",
        "      summ1.append('-' + i + '.')\n",
        "    # elif i[0] in ['M','T','O','A'] and (i[1].isalpha()==False):\n",
        "    #   id+=1\n",
        "    #   summ.append('')\n",
        "    #   summ[id] = summ[id] + ' -' + i + '.'\n",
        "    # elif i[0]=='M' and i[1]=='U':\n",
        "    #   id+=1\n",
        "    #   summ.append('')\n",
        "    #   summ[id] = summ[id] + ' -' + i + '.'\n",
        "    else:\n",
        "      summ1.append(i + '.')\n",
        "\n",
        "  summ1 = insert_pronouns(summ1)\n",
        "  for i in summ1:\n",
        "    if i[1] == 'P' and i[2] == 'E':\n",
        "      id+=1\n",
        "      summ.append('')\n",
        "      summ[id] = summ[id] + ' ' + i\n",
        "    else:\n",
        "      summ[id] = summ[id] + '\\n  ' + i\n",
        "\n",
        "  if '' in summ:\n",
        "    summ.remove('')\n",
        "  summ = '\\n'.join(summ)\n",
        "  return summ\n",
        "\n",
        "def insert_pronouns(summ1):\n",
        "  len_sum = len(summ1)\n",
        "  for line_no, i in enumerate(summ1):\n",
        "    if '-' in i:\n",
        "      if len_sum-line_no <= 3:\n",
        "        rng = len_sum-line_no-1\n",
        "      else:\n",
        "        rng = 3\n",
        "      for k1 in range(rng):\n",
        "        st1, st2 = check_req(i, summ1[line_no+k1+1])\n",
        "        if st1:\n",
        "          summ1[line_no+k1+1] = summ1[line_no+k1+1].replace(st1, 'They')\n",
        "          summ1[line_no+k1+1] = summ1[line_no+k1+1].replace(\"They's\", 'Their')\n",
        "          summ1[line_no+k1+1] = summ1[line_no+k1+1].replace(\"They is\", 'They are')\n",
        "          summ1[line_no+k1+1] = summ1[line_no+k1+1].replace(\"They is\", 'They are')\n",
        "          summ1[line_no+k1+1] = summ1[line_no+k1+1].replace(\"They has\", 'They have')\n",
        "          summ1[line_no+k1+1] = summ1[line_no+k1+1].replace(\"They wants\", 'They want')\n",
        "  return summ1\n",
        "\n",
        "def check_req(line1, line2):\n",
        "  if ('-' in line1) and ('-' in line2):\n",
        "    st1 = ''\n",
        "    st2 = ''\n",
        "    for _ in range(8):\n",
        "      st1+=line1[_]\n",
        "      st2+=line2[_]\n",
        "    if st1 == st2:\n",
        "      if line1[_+1] == line2[_+1]:\n",
        "        if line1[_+1]==' ':\n",
        "          st3 = st1\n",
        "          st4 = st2\n",
        "        elif line1[_+1]==',':\n",
        "          st3 = False\n",
        "          st4 = False\n",
        "        else:\n",
        "          st3 = st1+line1[_+1]\n",
        "          st4 = st2+line2[_+1]\n",
        "      else:\n",
        "        if line1[_+1]==\"'\" or line2[_+1]==\"'\":\n",
        "          st3 = st1\n",
        "          st4 = st2\n",
        "        else:\n",
        "          st3 = False\n",
        "          st4 = False\n",
        "    else:\n",
        "      st3 = False\n",
        "      st4 = False\n",
        "  else:\n",
        "    st3 = False\n",
        "    st4 = False\n",
        "\n",
        "  return st3, st4 \n",
        "\n",
        "def gen_summary(tscs_preprocessed):\n",
        "  s2 = []\n",
        "  filename = []\n",
        "  for k, v in tscs_preprocessed.items():\n",
        "\n",
        "    k = k.replace('meeting', 'minute')\n",
        "    k = k.replace('_transcript', '')\n",
        "    filename.append(k)\n",
        "\n",
        "    if len(v) < 11:\n",
        "      section = 2\n",
        "    elif len(v) < 18:\n",
        "      section = 4\n",
        "    elif len(v) < 24: \n",
        "      section = 6\n",
        "    else:\n",
        "      section = 8\n",
        "    s1 = ['']\n",
        "    tsc = v\n",
        "    id=0\n",
        "    for i, t1 in enumerate(tsc):\n",
        "      a1 = summarize(t1)\n",
        "      s1[id] = s1[id] + a1 + ' '\n",
        "      if i%section==0:\n",
        "        s1.append('')\n",
        "        id+=1\n",
        "\n",
        "    s2.append(s1)\n",
        "  return s2, filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCccJfVULOUR"
      },
      "source": [
        "### LOAD THE DATA ... A DICTIONARY :- key : meeting_id(str) ; value : roles(list), utterance(list) ###\n",
        "\n",
        "import json\n",
        "import re\n",
        "with open('/content/drive/MyDrive/AutoMin-2021/test.json', 'r') as out:\n",
        "  t = json.load(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6_e2PWcU30k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ff8690-fac4-4bba-cf2f-4f0a5c90a6ed"
      },
      "source": [
        "### IF INFERENCING ON A SPECIFIC TRANSCRIPT, INPUT THE MEETING ID... ###\n",
        "m_id = 'minutes_en_test_001'\n",
        "\n",
        "### IMPLEMENTING THE BELOW LINES WILL GIVE 3 SUMMARIES WITH VARYING LENGTHS, AS MENTIONED ###\n",
        "# tscs_preprocessed3, attendees = gen_tscs(m_id, 512) #for longer summary\n",
        "# tscs_preprocessed2, attendees = gen_tscs(m_id, 768)\n",
        "tscs_preprocessed1, attendees = gen_tscs(m_id, 1024) #for shorter summary\n",
        "\n",
        "print(len(tscs_preprocessed1))\n",
        "print(tscs_preprocessed1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "{'minutes_en_test_001': [\"PERSON5: Hi, how are you? Good morning.\\nPERSON13: Good morning. I'm, well, fine. Still at home.\\nPERSON13: You know. And you?\\nPERSON5: I'm also at home. But, ehm, the Czech republic government, they already lifted the Kind of lifted the rules. So, ehm, from this Monday we can actually go out even if it's not like the necessity. It the meetings of. Up to 10 people are allowed.\\nPERSON13: Ha. So here in LOCATION1, we have to wait until the 4th of May, some commercial activity can already be open. But we think that we have to wait until June for the free circulation of people. And fortunately, starting from the 4th of June we are allowed to reach our family. If it's in the same region.\\nPERSON13: And so finally, I will reach my parents. Cause we live in two different cities. Because I'm in Trento and my family is in Bolzano. Which is pretty near around 50 kilometres. But, but\\nPERSON5: So you're looking forward.\\nPERSON13: exactly. Hi guys, good morning.\\nPERSON5: So I went to the park yestreday and I was really happy. I mean I kept my distance from everyone but like it was green there. So it was.\\nPERSON9: But you can't go to the forest or something?\\nPERSON9: You can't just go to the forest or something?\\nPERSON5: Well, the rules changed here and since this Monday we can't go out even if it is not like the most necessary groceries or stuff like this, and.\\nPERSON9: And thanks we are able to go out to the forest, whenever we wanted.\\nPERSON5: Yes, but if you are in the city then you have to somehow get to the forest, so.\\nPERSON6: Hello. Can you hear me?\\nPERSON6: Sorry for the delay. So, Am I the last one?\\nPERSON5: Probably yes, because ORGANIZATION8,PERSON8 and PERSON14 said that they have the call so they can't join ours.\\nPERSON5: And ORGANIZATION8 is not coming.\\nPERSON6: So we have every partner. A partner from everywhere, that thats great. Thanks for joining. So this is again one of our regular calls. There will be, before the summer, there will be at least one more, in May. Oh no, two more actually. One in May and one in June, and. Let maybe PERSON5 should start with the administrative part. . So, PERSON5, can you? You know the review day, then all this.\\nPERSON5: Okay, so, we. PERSON12 told us that we will have to postpone one of our regular meeting. And she asked us about dates. So I already set it up. Well. Not many people voted in that Doodle. So, please vote for the postpone review. It should be in September. I know that nobody knows so litheraly if will be able to travel somewhere, and it looks like that it will be just online meeting. But, anyway just vote in a Doodle so I can get PERSON12 some dates. So, just do your best estimates for September. And that's it for that. We have we will have eh couple of deliverables due in June. I've link there in the in that Google sheet we have for tricking our like deliverables or stuff. We have the links for overview. So for these six up coming once you can basically start working on them. We haven't decided who will be our internal review person for any of them.\\nPERSON6: Which is a good opportunity to choose them now.\\nPERSON5: Yes. So just have a look. And if it's not your deliverable and you would like to read it just tell me.\\nPERSON6: So we will be contribute. So I think that the the two point one, the initial ASR systems ORGANIZATION6 is not providing in ASR system. And that's a technical We are providing some. So it will be good if if ORGANIZATION6 review that. So actually PERSON3 the. The volunteer should be entered in the continues reporting. She not not in this table just copy copypasted.\\nPERSON5: You can you can enter it here and I will just put it into the right place. You can just.\\nPERSON1: Okay, I'm gonna last review the summarization. But generally, there ASR also interesting for me. So probably put down PERSON3.\\nPERSON6: So I don't know, who should we the summarization eh review. Then that's another.\\n\", \"PERSON1: I think we're not contributing that don't know facts.\\nPERSON1: There we should wouldn't not this stage.\\nPERSON15: Okay, I would be happy to do the review.\\nPERSON6: Okay, rage. What can we review? Is there anything that we can review?\\nPERSON1: That's a problem we contribute everything.\\nPERSON6: And for this D7 2 do I understand correctly, PERSON5, please correct me. I think that we should ask for an extension for this deliver because Congress has not taken place.\\nPERSON6: So actually, I hope that this one will not be needed. So.\\nPERSON5: Yes, we will have to chec check it with PERSON12.\\nPERSON1: So, who is gonna review three point one, cause I'm hoping that everyone only the universe is contributing three point one.\\nPERSON1: So maybe maybe ORGANIZATION4 would be just a placed.\\nPERSON6: Exactly Because ORGANIZATION4 is also developing their own systems in this area.\\nPERSON6: There. So it would be great if ORGANIZATION4 did it.\\nPERSON16: Let me down the check, okay?\\nPERSON6: So PERSON16 should we list your name or not? Or you will let us know?\\nPERSON16: Eh, I will let you know.\\nPERSON6: okay. So lets put PERSON16 with two question marks. Okay. And then, then the management guy.\\nPERSON5: Well, I think I could I could. Well, the dissemination activities I could do that but if we are not. But I'm also doing the dissemination on the web and stuff like that.\\nPERSON6: I don't think there will be too too much of a problem. So I think that that that little should be like cross reviewed between ORGANIZATION8 and us. So we should review what they wrote and they should review what we wrote.\\nPERSON6: So, PERSON5, write yourself as the reviewer and\\nPERSON5: So I will put here PERSON5 plus PERSON8.\\nPERSON6: and I have no idea of this project management guide is again.\\nPERSON6: I think that will be checked an update.\\nPERSON5: No no, there is no, there wasn't any, anything like that before.\\nPERSON6: Oh, this is the.\\nPERSON5: Sum it. And this is the first one. So I'm apparently writing it.\\nPERSON5: And it will be a lot of lot of words about nothing but yes. Yes.\\nPERSON6: I´m afraid so.\\nPERSON5: So who wants who wants to read a fairy tale.\\nPERSON6: So the the best position would be I think ORGANIZATION6, because they have coordinated a number of projects.\\nPERSON1: I'm much rather read about ASR systems.\\nPERSON1: not that keen on reading it to be honest.\\nPERSON1: I could, you know, I could say we have to if it's not many words. PERSON5 said it'll be lots of words.\\nPERSON6: Yes, PERSON5, please cut down the number of words.\\nPERSON5: Like I think will run out of idea what to write there. So it won't be really long.\\nPERSON6: But there must be, there must be some project management guide from previous project.\\nPERSON5: I don't have it. So if you do you send to me.\\nPERSON1: I, I do not know, I think we managed to avoid such a thing before.\\nPERSON5: Why do we have it even?\\nPERSON1: Even I do remember writing it on.\\nPERSON6: well. Wasn't there anything like that from QT21 for example? I'll search.\\nPERSON5: I mean. Yesterday I search in sigma like what is the description of this of this deliverable, and it says just czech management guide. There is no description.\\nPERSON1: And this is the first iteration of that, is it?\\nPERSON1: And how it could be managing without project management guide so far if we don't. 15 months.\\nPERSON5: I mean, with PERSON6 we are awesome obviously. We can manage the project even without a guide.\\nPERSON6: it's just a party, they do do what they're expected to do even without the managing them. So that.\\nPERSON9: An exercise is so organized.\\n\", \"PERSON6: exactly, that's that's what PERSON2 can has always said I also believe in selforganization. Hm so QT21 doesn't seem to have this. They have this period reports and data management plans but not the project management guides.\\nPERSON1: I don't remember such a thing, no.\\nPERSON6: So it probably it was copypasted from something from somewhere. And no whatever.\\nPERSON1: I don't remember.\\nPERSON5: Okay, anyway, I'll write something and someone then will review it.\\nPERSON6: So another thing is like who would like to coordinate future projects. Those people should also have some a little incentive to to read it. So is any partner planning to do like start. Anyone who was not coordinated the projects yet. We could even ask ORGANIZATION8 to to review that because there. So that they would finally know what like what these EU projects are about. .\\nPERSON5: Okay, we can decided slowlier.\\nPERSON6: So it is important that they do start writing and the the due date has not been shifting. So that would be the end of June. So the internal reviews should be ready by mid June at the latest. So that we have two weeks to finish that.\\nPERSON1: Can you give us a date? Mid June seems kind of late.\\nPERSON6: Okay, yes, exactly. , let's do it earlier.\\nPERSON1: Especially things usually slept.\\nPERSON6: Yep So the 8th, the 8th of June.\\nPERSON1: that sounds fine.\\nPERSON6: So that should be the end of the review, kind of, right? Or no. It shou No, sorry, sorry. The beginning, the beginning. This should be delay.\\nPERSON1: It is the first draft.\\nPERSON6: First draft, yes, but complete draft. , So then you basically have a week reviewer two week to fix it.\\nPERSON1: And a week for no further every week spare\\nPERSON1: For for final tracks from the coordinator.\\nPERSON6: okay, that looks good. So, then the milestone. So the previous milestones of talk to PERSON4 in the email that you seen that of the milestones claiming that we have all the complete set up for ORGANIZATION7 Congress, which technically we have. So And there is one more milestone, the Congress. And PERSON4 said that there is no need to like think that of specifically, because, well, it won't happen yet. So we will take it off in that year from from now. So.\\nPERSON5: Yes and another. . Another another the most earlier milestone is in December. So. Do we find have anyone milestone in December. And there will be. I forgot to put the the one more deliverable. That will be due in the end of August. And that's the year 2 test set. So, that will be.\\nPERSON6: So the test sets, this is something that we are building. But unfortunately, the the colleague of mine who is supposed to be responsible for this is extremely slow. We call it the PROJECT1 test sets. So it is, I think the the main responsible for this deliverable is ORGANIZATION6, right?\\n\", \"PERSON6: So PERSON2 please, I have started this PROJECT1 test sets some time ago. You know, where the repository lives. So please pick up on that, and hopefully will not be like too confused from the layout of the of that test set and also that you would not disagree too much with the ambition that I have there. So it's, so this is this is technical thing no that me and PERSON2 should discussed. But, like my my idea is that we should have this populated and described by the August date. So that the we can then easily like submit as a deliver. So if the if you want test sets are not part of this yet and I think they should be. so wish put them in, and we should put all the other languages, and and everything. And we should also tested with our pipelines regularly. And the layout for for those who are not following these details these the layout of the test sets is that. It's many documents, and depending on the availablity of the document. Some of the languages are available. Sometimes it's also ASR test set. So, sometimes there is also the speech. Sometimes that just runs quick. Sometimes there is, there is the. Eeeh, that that was PERSON11, sorry. There is another meeting happening and I can't answer him now. So, it is and kind of assorted collection of documents in terms of languages, and like modality the speech, or or the text. And whenever someone wants to test against it. They will select a subset of files which have the require set of languages. So the test sets consists of these raw documents, obviously, curated to to serve well, linebay, segmented, everything. But the actual set of documents that you will test against will depend on the set of languages that you want to test. So the part of this repository is directory of file lists. And these file lists are then the subset of of that.\\nPERSON1: So this is the WMT 20 PROJECT1 test you are talking about, or?\\nPERSON6: This is, this is PROJECT1 test set. Do you see that?\\nPERSON6: So please, this is like the the starting point. And I have PERSON10 PERSON10 just should be really of like of coordinating annotators. The annotators are searching for poll documents and in many of the languages. We've asked that we have to have included more people we have any more people to the language map. And we have short term agreements with them. Unfortunately, PERSON10 is not feeding them with the prepared automatically paralyzed files. so that they would review them, because he is very slow, and he is also not soliciting new links from them. So that is it is. It is a growing much slower than I wanted to. We have enough time for August. But if we were really wanting this for the ORGANIZATION7 Congress we will be in in a bit of trouble.\\nPERSON1: So wa was the August the August just the low deliverable of the year 2 test set, is that right?\\nPERSON1: okay. And these are, okay So these are documents or sentences or speech. It is everything really?\\n\", \"PERSON6: It is everything, but if everything is curated. So it is like constraint, when when when finished, they all should be curated for the part particle purpose. So if it's the speech type of test set. It will be the sound, and the transcript with timestamps of individual words. That's what we are trying. We called it forced alignment. Sometimes we have to do it manually because the the the forced alignment automatic one fails. And if it's the machine translation test set then it's the standard poll think, ideally documents. We are trying to have documents, but sometimes it is the line oriented. So it needs to some still some clarification, like how do we like name the files. And then there is the file lists. And ideally, there would be automatic checks so that the everybody could check out this repository and run these checks. And then, with. It would check the number of lines, the the length of the non emptiness, and everything all the format things. And also it would automatically be able to create the file lists. So the file list should be essentially like fine graph. you you for those who like a clever fine graph so that you list all the files, and check that you have all the languages that you want. And with this fine graph the result of the set of files that that you can test on. And that should be stored there as a fixed file list, so then in order to evaluate with the test set you would say I'm for the evaluation I use this particular version of my model, whatever, and I use this commit ID of PROJECT1 test set with this file list. And that uniquely defines what is what is the test set. And obviously, some of these subsets would be like stable ones. So that we we would really have like there could be a file list cord called year 2 SLT test set. And year 2 empty test set and year 2 ASR test set. But that it would gradually grow as we will be covering more and more languages.\\nPERSON6: So it's. It can be seen as an overkill but.\\nPERSON1: And could be so suffered text to text can I do someone like sacrebleu or use well.\\nPERSON6: So this is this defines just the data and not the evaluation metric and that's a separatelly.\\nPERSON1: That, that and some some as a true, but sacrebleu has a way to just yet be a text.\\nPERSON6: So that would be a nice extension of sacrebleu, so that we could like add like FLAC to sacrebleu PROJECT1 test set and then the the file list name, and it would do automatical downloaded and it would put the commit ID the current commit ID into the fingerprint.\\nPERSON1: and I, I have observed that Max apps test sets really quickly to sacrebleu. Okay, whether he was not all test set from PERSON12. I'm not sure.\\nPERSON9: I mean, it's a little.\\nPERSON6: And so so the I think it would be better to kind of avoid forks, because then the versioning is is confused. But I'm totally agree that the public use of these test sets, the should be limited to few of those. And I think that the file list are the concept to use for this. So the the file list, and, that there should be only 3 at most 3 file lists that are interesting for the general public.\\nPERSON6: But we need, whatever we need the particle, computational linguistics domain. We need the auditing domain. We need the text audits versus speech audits domain.\\nPERSON6: So I think that for the for the general public there should be just at most 3 file lists. They could grow in time, the fingerprint would then it clearly indicate which which version was used. And it would be downloadable by sacrebleu automatically. I can't hear you. There u something happen. .\\nPERSON1: That all seems good. Emm, I mean, I mean, the sacrebleu, I mean, I was insane added sacrebleu. I just mean this kind of approach is very useful cause it makes it very easy.\\nPERSON6: So for the evaluation it's for the tool, we're still working on the SLTF, which is a private repository and you can see it, but it will be public wants finally finalize it. And it's it is now under like have it testing for the audible SLT share task evaluation.\\n\", \"PERSON6: So, we will know how the various scores behave. And it's this is geared towards the evaluation of spoken language translation, and it has kind of a mode, where it doesn't need a translation. So it's only ASR evaluation. The SLTF, ideally would be the sacrebleu for spoken English translation.\\nPERSON1: okay. I mean that. This is more interesting to me, because mean, essentially, I know I know how to text evaluation marks. And I know that's you just have some test sets you run you run whatever. Spoken language translation is a bit more but nearer to me. And then we have these these problems are and simultaneous translation. , of an effect on the button translation.\\nPERSON16: people I try to reconnecting if you needs.\\nPERSON6: Okay. . PERSON9. , PERSON9 please go on.\\nPERSON9: spoken translation evaluation effect to reward the same unless you need symetrics.\\nPERSON6: We do include latency. So SLTF does include delay and latency and and also the well the wasted effort. So there is two measures of wasted effort.\\nPERSON9: I haven't done that task on IWSLT to you.\\nPERSON9: It was boring thing integrate the model itself to the wrong model and and watch the output or.\\nPERSON6: No it trust it trust the it trust the time stamps given by the participants. And in our case, we actually all we said that the primary evaluation will be the translation quality, and the ASR and translation quality regardless the delaying. So we also have submissions who do not include any time stamps, the the the others. So it is like a secondary thing. It was not not a requirement for the audible SLT task this year. But I can imagine then in the next year, we would maybe even try running the model, so that this is a this is hard to run the models. But maybe we would really force everyone to submit time stamp information.\\nPERSON6: So this is, for us, we need it in any case. So the SLTF should serve us like what is ideal set up. Evaluating latency.\\nPERSON9: I have strong preference not to submit my model to eh\\nPERSON9: To organizers to run it for one, because the unpublished code\\nPERSON9: Because it's python code I have no choice, but to send the actual source code. And yahoo it's not a problem and, I would prefer just send the sour eh the lock file we have to do, because with the something in translation task and probably the SLT and I was not able do that.\\nPERSON1: Okay, but I mean, I mean how else do you. I mean if you want to save varieties these places is cautios so now, do you? How do you set this up?\\nPERSON9: I mean, so used that's for lock former time stamps.\\nPERSON6: Yes, this is exactly what we did for these for desirability to tasks it. It is quite clear, this cries what the lock file shouldn't lying.\\nPERSON6: And then you trust the the participants.\\n\", \"PERSON6: And there could be many lay. So this is all is risky with the lock files, because people still can misinterpret what time stamps should they use. So we try to be very clear about like this is the time when the the award was starting to be authored. This is the time when the word stopped to be authored. And this is the time when its ASR was available like printed on the screen for the user. And this is the time when the translation appeared on screen for the user. But people can misinterpret and also the the forced alignment that we're running. Eh, so so the so the the one people misinterpret it then someone's results can be like shifted in bad ways. So these measures will be always on the reliable, the only way to to do the comparison really fairly is to run the models or a serve the model. So that people with really live, receive the the sound, and they would like in in in actual networks sockets provide the the outputs. So this would be the only reliable way to to measure that. The the extra thing that I wanted to mention is that the forced alignment, which finds the words in the in the sound is not reliable for us either. So sometimes it is really shifted. Sometimes it is towards the ends of the word. Sometimes it is toward the beginning of the words. It's in your own network model that like attention somewhat flows. It's not attention but it still somewhat floats, floats around. And the the the only thing that the one can say to this is the this at least affects everybody the same way. When people misinterpret what the locks should be. Then everybody's like each party is affected in a different way, and that's bad for the for the evaluation. But I think that's like that just life. So I think it is, it is quite easy to proceed with a with these limitations and what the set up. So so the so ORGANIZATION6 people please review the PROJECT1 test set, as I set it up and please contribute to it in any possible way. You remember that some at the several months ago of of I suggested that you can ask your students and and PERSON3 said that well we cannot expect volunteers eeh possibly paid volunteers or nonpaid volunteers that that makes some different, but not to it to to just do slavery task on the data. Maybe you can find someone who can do it if you can paid them. That's okay, I'm not curious.\\nPERSON1: Sorry just reminded the task is what? One is cur is checking the translations are making translations or?\\nPERSON6: We do both. So.\\nPERSON6: So well, we we plan to do both we. We try to, find, and the and the revised translations.\\nPERSON6: And and this, if I was correct PERSON5 I do not know whether you have double checked my numbers. But it turns, it seems that finding and curating is half the price when we pay what we pay to our annotators normally compared to like the professional translation. So there is some reduction of costs but is. It's not like it doesn't come for free anyway.\\nPERSON6: One could, one could. , it is war. And one one could just trust the automatic processing and maybe use some quality automatic quality measures. So one could could cut it down further. But for now, I prefer, especially because is this kind of obscure languages. I prefer to find people who actually speak them, and I preferred them to to find the related data sets. But if we are unable to to find speech domain and auditing domain paralel??. Then indeed, we will ask the people to find these text monolingually and translated maybe back to Czech. So do manual back translation to Czech. This is the the wrong direction of translation. But it's the it's more reliable with respect the domain of interest.\\nPERSON1: In in terms of finding the translation I mean we and PERSON2 maybe can comment. We made some progress in getting translations out of the auditing websites, ehmm.\\nPERSON1: And that if that is what we are looking for pro tests about is nonspeech segments, that some text insistent. This is that.\\nPERSON6: So this is relevant. And this is this is what we really should do. the current people, those that we have have just like signed up work agreements, the short term contracts with, our four languages, which are not well represented there, so far. But we also want to do it for the well represented languages, because we should we should cover them as well. So so PERSON10 sh PERSON10 has, for example, link to one great site of of speeches and he should run by textor and they and he.\\nPERSON1: We can also work on this and for a lot of Irish state. If you are under represent to languages.\\n\", \"PERSON1: The Irish supreme auditor, apparently translates lot everything into Irish.\\nPERSON1: And doesn't very. It wasn't very structured way wasn't it though?\\nPERSON15: we got, ehm, what was it? And ten tens of thousands of sentences I think. .\\nPERSON1: We assume that Irish was not huge priority for the project But.\\nPERSON6: Well, Irish is equally equal priority with as other languages.\\nPERSON6: well, the project was started when the EU still existed. So we will see what happened.\\nPERSON5: Okay, there is one more point and we promised half hour call so.\\nPERSON6: Yes, with this is that the finishes, the set of feel free to step in, and whatever you can do for PERSON10 that will help us then the next person when PERSON10 is still not like woken up is PERSON7 who is now finishing the overview of the audible SLT test set. And he will be moving to to to these like supervision and managing the the annotators for the PROJECT1 test set if if PERSON10 doesn't start really. And, but feel free to step in and provide feedback on the layout, upload data sets everything. So let's let's get this grow. And if we if we do this over the following couple of weeks. Then and if we test with these test sets then it will be very easy to do the deliverable for August. . And the last point now, that's the demo. So here I would I was I was hoping that PERSON16 would would reconnect by this time. And yes maybe he is here. I'm not sure.\\nPERSON16: Yes I'm here.\\nPERSON6: So would it be possible that that ORGANIZATION4 would would like manage and make sure that this demo is delivered? As as the integration part. Obviously, like it's we will be PERSON11 will be running the systems. But I I need someone to to make sure that these things happen. Because I'm like overloaded and you're the integration partner. So that's that's the the general question at the beginning. And maybe let's wait with the answer until I tell you whatever we know about this. there is no particular requests for the scenario. As far as I know PERSON4 has just answered and they have also provided some feedback and that's interesting for everybody. So they've they've reminded us off BBC guidelines and standards for subtitling, which we are aware off, but they are not reflected in our systems in in any way. And then the better captioning or as spoken text translation ehm on screen will make better translation and readability, recomposing sentence on the fly my ethic liability. So so in a sense this is not the first time I hear that the users are always afraid of what ORGANIZATION2 worked on for for the past years. So ORGANIZATION2 has been working on this retranslation approach. And the users seem to, to prefer a delay. Ty buď zticha. Jo, ty neruš. Co potřebuješ? Cože? Řekni. Sundat povlečení. Jo, potom, prosim tě. To zvládnem pozdějc. Zkus to ty. , okay, so so maybe maybe ORGANIZATION2 has already experience with defending their approach to the users. I keep.\\nPERSON1: This is the segmentation as opposed to the, you mean, the segmentation and\\nPERSON9: ASR have focused on resending.\\nPERSON1: Ahh, okay, the retranslation, right.\\nPERSON6: The retranslation, yes. I think the users are afraid of retranslation because we were not able to hide it sufficiently. If we are able to make the retranslation stable. Then the users will not complain.\\nPERSON1: I mean, we've been playing with this and we can certainly improve the stability. I mean we only tested this simulator ASR which talking this morning about trying to get more testing the ASR. , if you're doing retranslation you're never going to be up to completely make it stable without actually messing up the end performs. It's sort of a trade off.\\nPERSON6: it is a it is a trade off. So. I think that in the long term I would like this to be evaluated on humans towards the end of the project we we we would really have like user study that would be great to to see which, and I think there will be people of different groups. Some will prefer this some will prefer that\\nPERSON1: Should be do this at the end or should be do this a bit sooner?\\n\", \"PERSON6: Any time we have the time for that.\\nPERSON1: Okay. Well, I mean it it's a question that's it's a question of making the time. I mean, it's whether we see it as a priority or not.\\nPERSON6: So it's that's a good idea. So we're now in the, in which of the project, it's where in the middle of the project, right.\\nPERSON1: I mean immediately end and say ou, we should retranslations a terrible idea. We should do less of it or we get the end and say retranslations is briliant. We should, you know, when you care about this. We we probably are not learn up before the end.\\nPERSON1: But I agree that way we for this. And I do not know.\\nPERSON9: Well, okay. So little bit respect on this, I'm not an ASR person so probably PERSON2 could more about this. But it's entirely possible that retranslation go go back a lot with the transition to endtoend ASR.\\nPERSON9: Because of course the the unstable have are based on which our research. Multiplies already to the high get models. So it's entirely possible that pressure answer. So because we decided to go to the end the data.\\nPERSON1: That you still have. We would still have instability. The MT is all also gonna inject instability, isn't it.\\nPERSON9: No. The MT only translate will be get from ASR. Like every hand is also changes, we have changes otherwise that we have.\\nPERSON1: so that injects instability. So the MT.\\nPERSON9: If there are no updated ASR hypothesis then they MT hypothesis.\\nPERSON1: Okay, so if the ASR wait to the end of the sentence before sending his hypothesis. Then obviously this know instability.\\nPERSON9: That's probably just going to get better as we train models in fact. We have lot.\\nPERSON9: We will have our new generation of eh models previously based transformers finally in direct translator. And so far the experience from actual lectures is that there is a big improvement.\\nPERSON6: Still I underst.\\nPERSON9: And this is how much improvements is for flickering and every translation support generally, the quality is.\\nPERSON6: Sorry, sorry. Go on. So I was I was double checking. PERSON9, you were saying that with the in the endtoend SLT which includes transform models now in the new generation. There is, no, no.\\nPERSON6: Okay, no internal SLT. , okay.\\nPERSON1: And endtoend ASR. I think he means.\\nPERSON6: Okay, endtoend ASR. So, endtoend ASR there is no partial sentences admitted. It would be only complete sentences admitted. Is that what you say?\\nPERSON9: because that's not my field. I think there is a lot of research right now.\\nPERSON9: Oh, I'm sorry. There is research going on right now. How to low latency, and when they ASR and right now. We do get a partial sentences, but I don't think the hypothesis updated as much.\\nPERSON1: So they don't change, but they still extend. You know it doesn't rewrite extends.\\nPERSON1: But even even extending could still could still lead flickering the MT it's like you listen to German. If you listen to German, you don't know what the verb it so you make prediction and reproduction wrong. German to English say, you don't know the verb is in the sentence predictor wrongly, or you could just wait, but maybe that's bad to.\\nPERSON6: Exactly. So this is, I think the the problem with the integration of the ASR and MT will remain even once the, the new generation of the ASR models is is there. And there will be the question for the users whether they preferred to wait for the German verb, or guess and put there some English verb. So there there would be a trade off like what and what confidence should I insert the verb and then maybe recovery. I'm sure that there is also ways in English in which you can still.\\nPERSON1: you can find examples and languages and say.\\nPERSON6: No, no, no. I mean, that you could recover from that\\nPERSON6: To preserve the stability and reintroduce some kind of correction.\\n\", \"PERSON1: So why I wonder this when you could to look what interpreters do. Cause they just have strategies for doing this, the massive strategies where they serve formulate the the speech and serve wheter the is open, but I don't know the computation like. And then there is the other aspect, which I think will be trouble for us. Like the system, even guess is what people are gonna to say, and translates sooner. And sometimes you can because you know the. So where is he asked for other word words. Sometimes You can guess to 90 percent. Yet should you do that? Because you may have maybe wrong.\\nPERSON6: So this is. We are trying to run GPT tool to predict the tail of the sentence.\\nPERSON6: But it so far. It's like, the ASR so bad that that the prediction is like totally off, and so far it doesn't work at all, but but we are trying this guessing. So maybe I think it totally makes sense to do this guesses. That's what the interpreters do. And that's a question whether we will be able to do it well enough and half a good enough confidence eh explicitely in the models to make the decision, whether we should follow this guess or not.\\nPERSON6: Okay. So back back to the demo.\\nPERSON6: So the demo. The date should be. Where was that idealy May? 14th May 17th the sooner the better. And there eh is there is all this, all these recommendations for all the partners and their systems. But I think we should. There is no way to touch up on these topics before the demo. So my question back to PERSON16. Would could you, could ORGANIZATION4 supervise the the organization of the of the demo? Obviously, asking all the partners to have their systems ready. And and all that, but like doing the communication, so that that eh successfully. That that we delivered demo.\\nPERSON16: So we can. It's important is 17 than 4.\\nPERSON6: Say it again, it's a.\\nPERSON16: I mean it's important it's not a. Because it's too\\nPERSON6: so they said, they said sooner is better. So maybe maybe the best option send, so did closer to sending. So could it be like the 8th the so 11th seems like the average.\\nPERSON16: We learn about quit couple of proposal in the.\\nPERSON6: Maybe, maybe, so actually it won't be better if you could even create the Doodle Poll with time slots already for PERSON4. And directly con PERSON4. So like these are the time slots, which we are like offering. And then, well for all the partners it should be. .\\nPERSON16: I think it's better if we post the final this proposal informaly as a Doodle and then ask PERSON4.\\nPERSON16: So we will, we will learn set up the Doodle, send it to in both partners. Then then once agree share with.\\nPERSON6: And also propose what you prefer to demo because there is. You send, you have seen the email. I've sent a couple of a like a recorded demo most to PERSON4 and propose, and something specific. It's up to us. So well, decide.\\nPERSON13: I saw the demo you send project officer and she mostly seen the Monday seminary, em.\\nPERSON6: You mean the Czech talk, right?\\nPERSON13: I, think that probably we if you would like to present to the same thing we should then everything right now put the same set up without the audition??. It's also well tested by you. So it will be probably the safe solution. Might just, my only worry is about how to presents the results to the project officier. Because actually during the demo you project both the ORGANIZATION1 representation and the sub ORGANIZATION4 subtitles. In the future of the page on the projector in the class. And we will not vote to presente the same thing online to the project officer. Because actually are two different web pages, it's completly different.\\nPERSON6: So what we can do is like screen sharing and screen a broadcasting.\\nPERSON13: Ah, okay. It works.\\nPERSON6: That is that is an option. And and a question is, what should be the material that we are that we are subtitling.\\nPERSON6: I think it could be some low like French watching session.\\nPERSON13: Yes to the ASR domains.\\nPERSON1: That was quite challenging.\\n\", \"PERSON1: I'm not worrying about this, they have the same process on subtitling. And not really doing subtitling at the moment, we are doing transcription and translation, which is not subtitling by some kind of summarization. Sometimes. Which we won't do. It just about managing expectations.\\nPERSON6: That's true. So the translation, transcription and translation. And in that case, that's the paragraph you, which we can include in the demo as well. So I think the demo should demo both.\\nPERSON1: Hm. I think the idea screenshare is a good one. Just takes away one indicate.\\nPERSON6: But the the live aspect there. So what the, what was the challenge on the French watching session that we didn't understand the source language.\\nPERSON1: we didn't, to be honest, someone PERSON13 understood. What what the topic was. And you didn't really understand, and I didn't. You talk about the item, didn't. Understand so well, I have trouble with with it as well. So it was quite hard to follow have to met.\\nPERSON6: So that's that's a safer way of selling what we are doing. I've already\\nPERSON1: Some like a TED talk suppose might be. Little bit to save, is it?\\nPERSON6: No, no, It's not necessary to save. So it could be authentic stock, or something like that, so that it's not delivered in English, the prime language. So maybe if the primer language would be German, because ORGANIZATION2 has good models for this. And from German into English and from English into all the languages, TEDx talk.\\nPERSON1: Is the audio gonna be easier and gonna be clearer than news broadcast is gonna be easier for ASR.\\nPERSON6: I think that the segmentation would be a little bit easier. But, Well, I don't know. Sentence segmentation. So what so far in our experiments what kills the performance for the final user of the machine translation is the sentence segmentation.\\nPERSON9: we have new models now as well.\\nPERSON1: Is it just cause sent me I mean because fundamentally sentiments segmentation is really hard, because people just you're trying compose compose something that is not really there cause I'm not speaking in sentences.\\nPERSON6: I think that many of the. There are many errors. So yes, there is this hard concept. But still, there there are many cases where I would simply nowhere to put the full stop. And the system does not put it there.\\nPERSON1: Okay. So it is about partly by having better models.\\nPERSON6: So maybe maybe PERSON9 he could propose some German talks that are, on this and we we should test the whole set up via English into all the languages.\\nPERSON9: Yes,m. I can do that in next week, I can't that this week.\\nPERSON6: Say it again. It will be when?\\nPERSON9: It will be next week.\\nPERSON6: Yes So. So the communication about the the day should be already like that to start internally immediately. Then early next week, we should sent an email to the PERSON4 to truce, his date. And that would be towards the end of the the next week, we should be ready for that. And in the week from the 11th, the sooner the better. We should run it life for them. And before that we should run it for ourselves, right?\\nPERSON6: Okay so. Sorry for not managing the the half an hour. But as you see, my kids are really getting hungry, and i still need to peel the potatoes, and. Thanks for joining, and will be in very close touch for for the demo in the coming days.\\n\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrdb34lYxkYZ",
        "outputId": "d42b8f83-d9fe-4fef-d933-758adca5a2ed"
      },
      "source": [
        "tscs_preprocessed1[m_id]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"PERSON5: Hi, how are you? Good morning.\\nPERSON13: Good morning. I'm, well, fine. Still at home.\\nPERSON13: You know. And you?\\nPERSON5: I'm also at home. But, ehm, the Czech republic government, they already lifted the Kind of lifted the rules. So, ehm, from this Monday we can actually go out even if it's not like the necessity. It the meetings of. Up to 10 people are allowed.\\nPERSON13: Ha. So here in LOCATION1, we have to wait until the 4th of May, some commercial activity can already be open. But we think that we have to wait until June for the free circulation of people. And fortunately, starting from the 4th of June we are allowed to reach our family. If it's in the same region.\\nPERSON13: And so finally, I will reach my parents. Cause we live in two different cities. Because I'm in Trento and my family is in Bolzano. Which is pretty near around 50 kilometres. But, but\\nPERSON5: So you're looking forward.\\nPERSON13: exactly. Hi guys, good morning.\\nPERSON5: So I went to the park yestreday and I was really happy. I mean I kept my distance from everyone but like it was green there. So it was.\\nPERSON9: But you can't go to the forest or something?\\nPERSON9: You can't just go to the forest or something?\\nPERSON5: Well, the rules changed here and since this Monday we can't go out even if it is not like the most necessary groceries or stuff like this, and.\\nPERSON9: And thanks we are able to go out to the forest, whenever we wanted.\\nPERSON5: Yes, but if you are in the city then you have to somehow get to the forest, so.\\nPERSON6: Hello. Can you hear me?\\nPERSON6: Sorry for the delay. So, Am I the last one?\\nPERSON5: Probably yes, because ORGANIZATION8,PERSON8 and PERSON14 said that they have the call so they can't join ours.\\nPERSON5: And ORGANIZATION8 is not coming.\\nPERSON6: So we have every partner. A partner from everywhere, that thats great. Thanks for joining. So this is again one of our regular calls. There will be, before the summer, there will be at least one more, in May. Oh no, two more actually. One in May and one in June, and. Let maybe PERSON5 should start with the administrative part. . So, PERSON5, can you? You know the review day, then all this.\\nPERSON5: Okay, so, we. PERSON12 told us that we will have to postpone one of our regular meeting. And she asked us about dates. So I already set it up. Well. Not many people voted in that Doodle. So, please vote for the postpone review. It should be in September. I know that nobody knows so litheraly if will be able to travel somewhere, and it looks like that it will be just online meeting. But, anyway just vote in a Doodle so I can get PERSON12 some dates. So, just do your best estimates for September. And that's it for that. We have we will have eh couple of deliverables due in June. I've link there in the in that Google sheet we have for tricking our like deliverables or stuff. We have the links for overview. So for these six up coming once you can basically start working on them. We haven't decided who will be our internal review person for any of them.\\nPERSON6: Which is a good opportunity to choose them now.\\nPERSON5: Yes. So just have a look. And if it's not your deliverable and you would like to read it just tell me.\\nPERSON6: So we will be contribute. So I think that the the two point one, the initial ASR systems ORGANIZATION6 is not providing in ASR system. And that's a technical We are providing some. So it will be good if if ORGANIZATION6 review that. So actually PERSON3 the. The volunteer should be entered in the continues reporting. She not not in this table just copy copypasted.\\nPERSON5: You can you can enter it here and I will just put it into the right place. You can just.\\nPERSON1: Okay, I'm gonna last review the summarization. But generally, there ASR also interesting for me. So probably put down PERSON3.\\nPERSON6: So I don't know, who should we the summarization eh review. Then that's another.\\n\",\n",
              " \"PERSON1: I think we're not contributing that don't know facts.\\nPERSON1: There we should wouldn't not this stage.\\nPERSON15: Okay, I would be happy to do the review.\\nPERSON6: Okay, rage. What can we review? Is there anything that we can review?\\nPERSON1: That's a problem we contribute everything.\\nPERSON6: And for this D7 2 do I understand correctly, PERSON5, please correct me. I think that we should ask for an extension for this deliver because Congress has not taken place.\\nPERSON6: So actually, I hope that this one will not be needed. So.\\nPERSON5: Yes, we will have to chec check it with PERSON12.\\nPERSON1: So, who is gonna review three point one, cause I'm hoping that everyone only the universe is contributing three point one.\\nPERSON1: So maybe maybe ORGANIZATION4 would be just a placed.\\nPERSON6: Exactly Because ORGANIZATION4 is also developing their own systems in this area.\\nPERSON6: There. So it would be great if ORGANIZATION4 did it.\\nPERSON16: Let me down the check, okay?\\nPERSON6: So PERSON16 should we list your name or not? Or you will let us know?\\nPERSON16: Eh, I will let you know.\\nPERSON6: okay. So lets put PERSON16 with two question marks. Okay. And then, then the management guy.\\nPERSON5: Well, I think I could I could. Well, the dissemination activities I could do that but if we are not. But I'm also doing the dissemination on the web and stuff like that.\\nPERSON6: I don't think there will be too too much of a problem. So I think that that that little should be like cross reviewed between ORGANIZATION8 and us. So we should review what they wrote and they should review what we wrote.\\nPERSON6: So, PERSON5, write yourself as the reviewer and\\nPERSON5: So I will put here PERSON5 plus PERSON8.\\nPERSON6: and I have no idea of this project management guide is again.\\nPERSON6: I think that will be checked an update.\\nPERSON5: No no, there is no, there wasn't any, anything like that before.\\nPERSON6: Oh, this is the.\\nPERSON5: Sum it. And this is the first one. So I'm apparently writing it.\\nPERSON5: And it will be a lot of lot of words about nothing but yes. Yes.\\nPERSON6: I´m afraid so.\\nPERSON5: So who wants who wants to read a fairy tale.\\nPERSON6: So the the best position would be I think ORGANIZATION6, because they have coordinated a number of projects.\\nPERSON1: I'm much rather read about ASR systems.\\nPERSON1: not that keen on reading it to be honest.\\nPERSON1: I could, you know, I could say we have to if it's not many words. PERSON5 said it'll be lots of words.\\nPERSON6: Yes, PERSON5, please cut down the number of words.\\nPERSON5: Like I think will run out of idea what to write there. So it won't be really long.\\nPERSON6: But there must be, there must be some project management guide from previous project.\\nPERSON5: I don't have it. So if you do you send to me.\\nPERSON1: I, I do not know, I think we managed to avoid such a thing before.\\nPERSON5: Why do we have it even?\\nPERSON1: Even I do remember writing it on.\\nPERSON6: well. Wasn't there anything like that from QT21 for example? I'll search.\\nPERSON5: I mean. Yesterday I search in sigma like what is the description of this of this deliverable, and it says just czech management guide. There is no description.\\nPERSON1: And this is the first iteration of that, is it?\\nPERSON1: And how it could be managing without project management guide so far if we don't. 15 months.\\nPERSON5: I mean, with PERSON6 we are awesome obviously. We can manage the project even without a guide.\\nPERSON6: it's just a party, they do do what they're expected to do even without the managing them. So that.\\nPERSON9: An exercise is so organized.\\n\",\n",
              " \"PERSON6: exactly, that's that's what PERSON2 can has always said I also believe in selforganization. Hm so QT21 doesn't seem to have this. They have this period reports and data management plans but not the project management guides.\\nPERSON1: I don't remember such a thing, no.\\nPERSON6: So it probably it was copypasted from something from somewhere. And no whatever.\\nPERSON1: I don't remember.\\nPERSON5: Okay, anyway, I'll write something and someone then will review it.\\nPERSON6: So another thing is like who would like to coordinate future projects. Those people should also have some a little incentive to to read it. So is any partner planning to do like start. Anyone who was not coordinated the projects yet. We could even ask ORGANIZATION8 to to review that because there. So that they would finally know what like what these EU projects are about. .\\nPERSON5: Okay, we can decided slowlier.\\nPERSON6: So it is important that they do start writing and the the due date has not been shifting. So that would be the end of June. So the internal reviews should be ready by mid June at the latest. So that we have two weeks to finish that.\\nPERSON1: Can you give us a date? Mid June seems kind of late.\\nPERSON6: Okay, yes, exactly. , let's do it earlier.\\nPERSON1: Especially things usually slept.\\nPERSON6: Yep So the 8th, the 8th of June.\\nPERSON1: that sounds fine.\\nPERSON6: So that should be the end of the review, kind of, right? Or no. It shou No, sorry, sorry. The beginning, the beginning. This should be delay.\\nPERSON1: It is the first draft.\\nPERSON6: First draft, yes, but complete draft. , So then you basically have a week reviewer two week to fix it.\\nPERSON1: And a week for no further every week spare\\nPERSON1: For for final tracks from the coordinator.\\nPERSON6: okay, that looks good. So, then the milestone. So the previous milestones of talk to PERSON4 in the email that you seen that of the milestones claiming that we have all the complete set up for ORGANIZATION7 Congress, which technically we have. So And there is one more milestone, the Congress. And PERSON4 said that there is no need to like think that of specifically, because, well, it won't happen yet. So we will take it off in that year from from now. So.\\nPERSON5: Yes and another. . Another another the most earlier milestone is in December. So. Do we find have anyone milestone in December. And there will be. I forgot to put the the one more deliverable. That will be due in the end of August. And that's the year 2 test set. So, that will be.\\nPERSON6: So the test sets, this is something that we are building. But unfortunately, the the colleague of mine who is supposed to be responsible for this is extremely slow. We call it the PROJECT1 test sets. So it is, I think the the main responsible for this deliverable is ORGANIZATION6, right?\\n\",\n",
              " \"PERSON6: So PERSON2 please, I have started this PROJECT1 test sets some time ago. You know, where the repository lives. So please pick up on that, and hopefully will not be like too confused from the layout of the of that test set and also that you would not disagree too much with the ambition that I have there. So it's, so this is this is technical thing no that me and PERSON2 should discussed. But, like my my idea is that we should have this populated and described by the August date. So that the we can then easily like submit as a deliver. So if the if you want test sets are not part of this yet and I think they should be. so wish put them in, and we should put all the other languages, and and everything. And we should also tested with our pipelines regularly. And the layout for for those who are not following these details these the layout of the test sets is that. It's many documents, and depending on the availablity of the document. Some of the languages are available. Sometimes it's also ASR test set. So, sometimes there is also the speech. Sometimes that just runs quick. Sometimes there is, there is the. Eeeh, that that was PERSON11, sorry. There is another meeting happening and I can't answer him now. So, it is and kind of assorted collection of documents in terms of languages, and like modality the speech, or or the text. And whenever someone wants to test against it. They will select a subset of files which have the require set of languages. So the test sets consists of these raw documents, obviously, curated to to serve well, linebay, segmented, everything. But the actual set of documents that you will test against will depend on the set of languages that you want to test. So the part of this repository is directory of file lists. And these file lists are then the subset of of that.\\nPERSON1: So this is the WMT 20 PROJECT1 test you are talking about, or?\\nPERSON6: This is, this is PROJECT1 test set. Do you see that?\\nPERSON6: So please, this is like the the starting point. And I have PERSON10 PERSON10 just should be really of like of coordinating annotators. The annotators are searching for poll documents and in many of the languages. We've asked that we have to have included more people we have any more people to the language map. And we have short term agreements with them. Unfortunately, PERSON10 is not feeding them with the prepared automatically paralyzed files. so that they would review them, because he is very slow, and he is also not soliciting new links from them. So that is it is. It is a growing much slower than I wanted to. We have enough time for August. But if we were really wanting this for the ORGANIZATION7 Congress we will be in in a bit of trouble.\\nPERSON1: So wa was the August the August just the low deliverable of the year 2 test set, is that right?\\nPERSON1: okay. And these are, okay So these are documents or sentences or speech. It is everything really?\\n\",\n",
              " \"PERSON6: It is everything, but if everything is curated. So it is like constraint, when when when finished, they all should be curated for the part particle purpose. So if it's the speech type of test set. It will be the sound, and the transcript with timestamps of individual words. That's what we are trying. We called it forced alignment. Sometimes we have to do it manually because the the the forced alignment automatic one fails. And if it's the machine translation test set then it's the standard poll think, ideally documents. We are trying to have documents, but sometimes it is the line oriented. So it needs to some still some clarification, like how do we like name the files. And then there is the file lists. And ideally, there would be automatic checks so that the everybody could check out this repository and run these checks. And then, with. It would check the number of lines, the the length of the non emptiness, and everything all the format things. And also it would automatically be able to create the file lists. So the file list should be essentially like fine graph. you you for those who like a clever fine graph so that you list all the files, and check that you have all the languages that you want. And with this fine graph the result of the set of files that that you can test on. And that should be stored there as a fixed file list, so then in order to evaluate with the test set you would say I'm for the evaluation I use this particular version of my model, whatever, and I use this commit ID of PROJECT1 test set with this file list. And that uniquely defines what is what is the test set. And obviously, some of these subsets would be like stable ones. So that we we would really have like there could be a file list cord called year 2 SLT test set. And year 2 empty test set and year 2 ASR test set. But that it would gradually grow as we will be covering more and more languages.\\nPERSON6: So it's. It can be seen as an overkill but.\\nPERSON1: And could be so suffered text to text can I do someone like sacrebleu or use well.\\nPERSON6: So this is this defines just the data and not the evaluation metric and that's a separatelly.\\nPERSON1: That, that and some some as a true, but sacrebleu has a way to just yet be a text.\\nPERSON6: So that would be a nice extension of sacrebleu, so that we could like add like FLAC to sacrebleu PROJECT1 test set and then the the file list name, and it would do automatical downloaded and it would put the commit ID the current commit ID into the fingerprint.\\nPERSON1: and I, I have observed that Max apps test sets really quickly to sacrebleu. Okay, whether he was not all test set from PERSON12. I'm not sure.\\nPERSON9: I mean, it's a little.\\nPERSON6: And so so the I think it would be better to kind of avoid forks, because then the versioning is is confused. But I'm totally agree that the public use of these test sets, the should be limited to few of those. And I think that the file list are the concept to use for this. So the the file list, and, that there should be only 3 at most 3 file lists that are interesting for the general public.\\nPERSON6: But we need, whatever we need the particle, computational linguistics domain. We need the auditing domain. We need the text audits versus speech audits domain.\\nPERSON6: So I think that for the for the general public there should be just at most 3 file lists. They could grow in time, the fingerprint would then it clearly indicate which which version was used. And it would be downloadable by sacrebleu automatically. I can't hear you. There u something happen. .\\nPERSON1: That all seems good. Emm, I mean, I mean, the sacrebleu, I mean, I was insane added sacrebleu. I just mean this kind of approach is very useful cause it makes it very easy.\\nPERSON6: So for the evaluation it's for the tool, we're still working on the SLTF, which is a private repository and you can see it, but it will be public wants finally finalize it. And it's it is now under like have it testing for the audible SLT share task evaluation.\\n\",\n",
              " \"PERSON6: So, we will know how the various scores behave. And it's this is geared towards the evaluation of spoken language translation, and it has kind of a mode, where it doesn't need a translation. So it's only ASR evaluation. The SLTF, ideally would be the sacrebleu for spoken English translation.\\nPERSON1: okay. I mean that. This is more interesting to me, because mean, essentially, I know I know how to text evaluation marks. And I know that's you just have some test sets you run you run whatever. Spoken language translation is a bit more but nearer to me. And then we have these these problems are and simultaneous translation. , of an effect on the button translation.\\nPERSON16: people I try to reconnecting if you needs.\\nPERSON6: Okay. . PERSON9. , PERSON9 please go on.\\nPERSON9: spoken translation evaluation effect to reward the same unless you need symetrics.\\nPERSON6: We do include latency. So SLTF does include delay and latency and and also the well the wasted effort. So there is two measures of wasted effort.\\nPERSON9: I haven't done that task on IWSLT to you.\\nPERSON9: It was boring thing integrate the model itself to the wrong model and and watch the output or.\\nPERSON6: No it trust it trust the it trust the time stamps given by the participants. And in our case, we actually all we said that the primary evaluation will be the translation quality, and the ASR and translation quality regardless the delaying. So we also have submissions who do not include any time stamps, the the the others. So it is like a secondary thing. It was not not a requirement for the audible SLT task this year. But I can imagine then in the next year, we would maybe even try running the model, so that this is a this is hard to run the models. But maybe we would really force everyone to submit time stamp information.\\nPERSON6: So this is, for us, we need it in any case. So the SLTF should serve us like what is ideal set up. Evaluating latency.\\nPERSON9: I have strong preference not to submit my model to eh\\nPERSON9: To organizers to run it for one, because the unpublished code\\nPERSON9: Because it's python code I have no choice, but to send the actual source code. And yahoo it's not a problem and, I would prefer just send the sour eh the lock file we have to do, because with the something in translation task and probably the SLT and I was not able do that.\\nPERSON1: Okay, but I mean, I mean how else do you. I mean if you want to save varieties these places is cautios so now, do you? How do you set this up?\\nPERSON9: I mean, so used that's for lock former time stamps.\\nPERSON6: Yes, this is exactly what we did for these for desirability to tasks it. It is quite clear, this cries what the lock file shouldn't lying.\\nPERSON6: And then you trust the the participants.\\n\",\n",
              " \"PERSON6: And there could be many lay. So this is all is risky with the lock files, because people still can misinterpret what time stamps should they use. So we try to be very clear about like this is the time when the the award was starting to be authored. This is the time when the word stopped to be authored. And this is the time when its ASR was available like printed on the screen for the user. And this is the time when the translation appeared on screen for the user. But people can misinterpret and also the the forced alignment that we're running. Eh, so so the so the the one people misinterpret it then someone's results can be like shifted in bad ways. So these measures will be always on the reliable, the only way to to do the comparison really fairly is to run the models or a serve the model. So that people with really live, receive the the sound, and they would like in in in actual networks sockets provide the the outputs. So this would be the only reliable way to to measure that. The the extra thing that I wanted to mention is that the forced alignment, which finds the words in the in the sound is not reliable for us either. So sometimes it is really shifted. Sometimes it is towards the ends of the word. Sometimes it is toward the beginning of the words. It's in your own network model that like attention somewhat flows. It's not attention but it still somewhat floats, floats around. And the the the only thing that the one can say to this is the this at least affects everybody the same way. When people misinterpret what the locks should be. Then everybody's like each party is affected in a different way, and that's bad for the for the evaluation. But I think that's like that just life. So I think it is, it is quite easy to proceed with a with these limitations and what the set up. So so the so ORGANIZATION6 people please review the PROJECT1 test set, as I set it up and please contribute to it in any possible way. You remember that some at the several months ago of of I suggested that you can ask your students and and PERSON3 said that well we cannot expect volunteers eeh possibly paid volunteers or nonpaid volunteers that that makes some different, but not to it to to just do slavery task on the data. Maybe you can find someone who can do it if you can paid them. That's okay, I'm not curious.\\nPERSON1: Sorry just reminded the task is what? One is cur is checking the translations are making translations or?\\nPERSON6: We do both. So.\\nPERSON6: So well, we we plan to do both we. We try to, find, and the and the revised translations.\\nPERSON6: And and this, if I was correct PERSON5 I do not know whether you have double checked my numbers. But it turns, it seems that finding and curating is half the price when we pay what we pay to our annotators normally compared to like the professional translation. So there is some reduction of costs but is. It's not like it doesn't come for free anyway.\\nPERSON6: One could, one could. , it is war. And one one could just trust the automatic processing and maybe use some quality automatic quality measures. So one could could cut it down further. But for now, I prefer, especially because is this kind of obscure languages. I prefer to find people who actually speak them, and I preferred them to to find the related data sets. But if we are unable to to find speech domain and auditing domain paralel??. Then indeed, we will ask the people to find these text monolingually and translated maybe back to Czech. So do manual back translation to Czech. This is the the wrong direction of translation. But it's the it's more reliable with respect the domain of interest.\\nPERSON1: In in terms of finding the translation I mean we and PERSON2 maybe can comment. We made some progress in getting translations out of the auditing websites, ehmm.\\nPERSON1: And that if that is what we are looking for pro tests about is nonspeech segments, that some text insistent. This is that.\\nPERSON6: So this is relevant. And this is this is what we really should do. the current people, those that we have have just like signed up work agreements, the short term contracts with, our four languages, which are not well represented there, so far. But we also want to do it for the well represented languages, because we should we should cover them as well. So so PERSON10 sh PERSON10 has, for example, link to one great site of of speeches and he should run by textor and they and he.\\nPERSON1: We can also work on this and for a lot of Irish state. If you are under represent to languages.\\n\",\n",
              " \"PERSON1: The Irish supreme auditor, apparently translates lot everything into Irish.\\nPERSON1: And doesn't very. It wasn't very structured way wasn't it though?\\nPERSON15: we got, ehm, what was it? And ten tens of thousands of sentences I think. .\\nPERSON1: We assume that Irish was not huge priority for the project But.\\nPERSON6: Well, Irish is equally equal priority with as other languages.\\nPERSON6: well, the project was started when the EU still existed. So we will see what happened.\\nPERSON5: Okay, there is one more point and we promised half hour call so.\\nPERSON6: Yes, with this is that the finishes, the set of feel free to step in, and whatever you can do for PERSON10 that will help us then the next person when PERSON10 is still not like woken up is PERSON7 who is now finishing the overview of the audible SLT test set. And he will be moving to to to these like supervision and managing the the annotators for the PROJECT1 test set if if PERSON10 doesn't start really. And, but feel free to step in and provide feedback on the layout, upload data sets everything. So let's let's get this grow. And if we if we do this over the following couple of weeks. Then and if we test with these test sets then it will be very easy to do the deliverable for August. . And the last point now, that's the demo. So here I would I was I was hoping that PERSON16 would would reconnect by this time. And yes maybe he is here. I'm not sure.\\nPERSON16: Yes I'm here.\\nPERSON6: So would it be possible that that ORGANIZATION4 would would like manage and make sure that this demo is delivered? As as the integration part. Obviously, like it's we will be PERSON11 will be running the systems. But I I need someone to to make sure that these things happen. Because I'm like overloaded and you're the integration partner. So that's that's the the general question at the beginning. And maybe let's wait with the answer until I tell you whatever we know about this. there is no particular requests for the scenario. As far as I know PERSON4 has just answered and they have also provided some feedback and that's interesting for everybody. So they've they've reminded us off BBC guidelines and standards for subtitling, which we are aware off, but they are not reflected in our systems in in any way. And then the better captioning or as spoken text translation ehm on screen will make better translation and readability, recomposing sentence on the fly my ethic liability. So so in a sense this is not the first time I hear that the users are always afraid of what ORGANIZATION2 worked on for for the past years. So ORGANIZATION2 has been working on this retranslation approach. And the users seem to, to prefer a delay. Ty buď zticha. Jo, ty neruš. Co potřebuješ? Cože? Řekni. Sundat povlečení. Jo, potom, prosim tě. To zvládnem pozdějc. Zkus to ty. , okay, so so maybe maybe ORGANIZATION2 has already experience with defending their approach to the users. I keep.\\nPERSON1: This is the segmentation as opposed to the, you mean, the segmentation and\\nPERSON9: ASR have focused on resending.\\nPERSON1: Ahh, okay, the retranslation, right.\\nPERSON6: The retranslation, yes. I think the users are afraid of retranslation because we were not able to hide it sufficiently. If we are able to make the retranslation stable. Then the users will not complain.\\nPERSON1: I mean, we've been playing with this and we can certainly improve the stability. I mean we only tested this simulator ASR which talking this morning about trying to get more testing the ASR. , if you're doing retranslation you're never going to be up to completely make it stable without actually messing up the end performs. It's sort of a trade off.\\nPERSON6: it is a it is a trade off. So. I think that in the long term I would like this to be evaluated on humans towards the end of the project we we we would really have like user study that would be great to to see which, and I think there will be people of different groups. Some will prefer this some will prefer that\\nPERSON1: Should be do this at the end or should be do this a bit sooner?\\n\",\n",
              " \"PERSON6: Any time we have the time for that.\\nPERSON1: Okay. Well, I mean it it's a question that's it's a question of making the time. I mean, it's whether we see it as a priority or not.\\nPERSON6: So it's that's a good idea. So we're now in the, in which of the project, it's where in the middle of the project, right.\\nPERSON1: I mean immediately end and say ou, we should retranslations a terrible idea. We should do less of it or we get the end and say retranslations is briliant. We should, you know, when you care about this. We we probably are not learn up before the end.\\nPERSON1: But I agree that way we for this. And I do not know.\\nPERSON9: Well, okay. So little bit respect on this, I'm not an ASR person so probably PERSON2 could more about this. But it's entirely possible that retranslation go go back a lot with the transition to endtoend ASR.\\nPERSON9: Because of course the the unstable have are based on which our research. Multiplies already to the high get models. So it's entirely possible that pressure answer. So because we decided to go to the end the data.\\nPERSON1: That you still have. We would still have instability. The MT is all also gonna inject instability, isn't it.\\nPERSON9: No. The MT only translate will be get from ASR. Like every hand is also changes, we have changes otherwise that we have.\\nPERSON1: so that injects instability. So the MT.\\nPERSON9: If there are no updated ASR hypothesis then they MT hypothesis.\\nPERSON1: Okay, so if the ASR wait to the end of the sentence before sending his hypothesis. Then obviously this know instability.\\nPERSON9: That's probably just going to get better as we train models in fact. We have lot.\\nPERSON9: We will have our new generation of eh models previously based transformers finally in direct translator. And so far the experience from actual lectures is that there is a big improvement.\\nPERSON6: Still I underst.\\nPERSON9: And this is how much improvements is for flickering and every translation support generally, the quality is.\\nPERSON6: Sorry, sorry. Go on. So I was I was double checking. PERSON9, you were saying that with the in the endtoend SLT which includes transform models now in the new generation. There is, no, no.\\nPERSON6: Okay, no internal SLT. , okay.\\nPERSON1: And endtoend ASR. I think he means.\\nPERSON6: Okay, endtoend ASR. So, endtoend ASR there is no partial sentences admitted. It would be only complete sentences admitted. Is that what you say?\\nPERSON9: because that's not my field. I think there is a lot of research right now.\\nPERSON9: Oh, I'm sorry. There is research going on right now. How to low latency, and when they ASR and right now. We do get a partial sentences, but I don't think the hypothesis updated as much.\\nPERSON1: So they don't change, but they still extend. You know it doesn't rewrite extends.\\nPERSON1: But even even extending could still could still lead flickering the MT it's like you listen to German. If you listen to German, you don't know what the verb it so you make prediction and reproduction wrong. German to English say, you don't know the verb is in the sentence predictor wrongly, or you could just wait, but maybe that's bad to.\\nPERSON6: Exactly. So this is, I think the the problem with the integration of the ASR and MT will remain even once the, the new generation of the ASR models is is there. And there will be the question for the users whether they preferred to wait for the German verb, or guess and put there some English verb. So there there would be a trade off like what and what confidence should I insert the verb and then maybe recovery. I'm sure that there is also ways in English in which you can still.\\nPERSON1: you can find examples and languages and say.\\nPERSON6: No, no, no. I mean, that you could recover from that\\nPERSON6: To preserve the stability and reintroduce some kind of correction.\\n\",\n",
              " \"PERSON1: So why I wonder this when you could to look what interpreters do. Cause they just have strategies for doing this, the massive strategies where they serve formulate the the speech and serve wheter the is open, but I don't know the computation like. And then there is the other aspect, which I think will be trouble for us. Like the system, even guess is what people are gonna to say, and translates sooner. And sometimes you can because you know the. So where is he asked for other word words. Sometimes You can guess to 90 percent. Yet should you do that? Because you may have maybe wrong.\\nPERSON6: So this is. We are trying to run GPT tool to predict the tail of the sentence.\\nPERSON6: But it so far. It's like, the ASR so bad that that the prediction is like totally off, and so far it doesn't work at all, but but we are trying this guessing. So maybe I think it totally makes sense to do this guesses. That's what the interpreters do. And that's a question whether we will be able to do it well enough and half a good enough confidence eh explicitely in the models to make the decision, whether we should follow this guess or not.\\nPERSON6: Okay. So back back to the demo.\\nPERSON6: So the demo. The date should be. Where was that idealy May? 14th May 17th the sooner the better. And there eh is there is all this, all these recommendations for all the partners and their systems. But I think we should. There is no way to touch up on these topics before the demo. So my question back to PERSON16. Would could you, could ORGANIZATION4 supervise the the organization of the of the demo? Obviously, asking all the partners to have their systems ready. And and all that, but like doing the communication, so that that eh successfully. That that we delivered demo.\\nPERSON16: So we can. It's important is 17 than 4.\\nPERSON6: Say it again, it's a.\\nPERSON16: I mean it's important it's not a. Because it's too\\nPERSON6: so they said, they said sooner is better. So maybe maybe the best option send, so did closer to sending. So could it be like the 8th the so 11th seems like the average.\\nPERSON16: We learn about quit couple of proposal in the.\\nPERSON6: Maybe, maybe, so actually it won't be better if you could even create the Doodle Poll with time slots already for PERSON4. And directly con PERSON4. So like these are the time slots, which we are like offering. And then, well for all the partners it should be. .\\nPERSON16: I think it's better if we post the final this proposal informaly as a Doodle and then ask PERSON4.\\nPERSON16: So we will, we will learn set up the Doodle, send it to in both partners. Then then once agree share with.\\nPERSON6: And also propose what you prefer to demo because there is. You send, you have seen the email. I've sent a couple of a like a recorded demo most to PERSON4 and propose, and something specific. It's up to us. So well, decide.\\nPERSON13: I saw the demo you send project officer and she mostly seen the Monday seminary, em.\\nPERSON6: You mean the Czech talk, right?\\nPERSON13: I, think that probably we if you would like to present to the same thing we should then everything right now put the same set up without the audition??. It's also well tested by you. So it will be probably the safe solution. Might just, my only worry is about how to presents the results to the project officier. Because actually during the demo you project both the ORGANIZATION1 representation and the sub ORGANIZATION4 subtitles. In the future of the page on the projector in the class. And we will not vote to presente the same thing online to the project officer. Because actually are two different web pages, it's completly different.\\nPERSON6: So what we can do is like screen sharing and screen a broadcasting.\\nPERSON13: Ah, okay. It works.\\nPERSON6: That is that is an option. And and a question is, what should be the material that we are that we are subtitling.\\nPERSON6: I think it could be some low like French watching session.\\nPERSON13: Yes to the ASR domains.\\nPERSON1: That was quite challenging.\\n\",\n",
              " \"PERSON1: I'm not worrying about this, they have the same process on subtitling. And not really doing subtitling at the moment, we are doing transcription and translation, which is not subtitling by some kind of summarization. Sometimes. Which we won't do. It just about managing expectations.\\nPERSON6: That's true. So the translation, transcription and translation. And in that case, that's the paragraph you, which we can include in the demo as well. So I think the demo should demo both.\\nPERSON1: Hm. I think the idea screenshare is a good one. Just takes away one indicate.\\nPERSON6: But the the live aspect there. So what the, what was the challenge on the French watching session that we didn't understand the source language.\\nPERSON1: we didn't, to be honest, someone PERSON13 understood. What what the topic was. And you didn't really understand, and I didn't. You talk about the item, didn't. Understand so well, I have trouble with with it as well. So it was quite hard to follow have to met.\\nPERSON6: So that's that's a safer way of selling what we are doing. I've already\\nPERSON1: Some like a TED talk suppose might be. Little bit to save, is it?\\nPERSON6: No, no, It's not necessary to save. So it could be authentic stock, or something like that, so that it's not delivered in English, the prime language. So maybe if the primer language would be German, because ORGANIZATION2 has good models for this. And from German into English and from English into all the languages, TEDx talk.\\nPERSON1: Is the audio gonna be easier and gonna be clearer than news broadcast is gonna be easier for ASR.\\nPERSON6: I think that the segmentation would be a little bit easier. But, Well, I don't know. Sentence segmentation. So what so far in our experiments what kills the performance for the final user of the machine translation is the sentence segmentation.\\nPERSON9: we have new models now as well.\\nPERSON1: Is it just cause sent me I mean because fundamentally sentiments segmentation is really hard, because people just you're trying compose compose something that is not really there cause I'm not speaking in sentences.\\nPERSON6: I think that many of the. There are many errors. So yes, there is this hard concept. But still, there there are many cases where I would simply nowhere to put the full stop. And the system does not put it there.\\nPERSON1: Okay. So it is about partly by having better models.\\nPERSON6: So maybe maybe PERSON9 he could propose some German talks that are, on this and we we should test the whole set up via English into all the languages.\\nPERSON9: Yes,m. I can do that in next week, I can't that this week.\\nPERSON6: Say it again. It will be when?\\nPERSON9: It will be next week.\\nPERSON6: Yes So. So the communication about the the day should be already like that to start internally immediately. Then early next week, we should sent an email to the PERSON4 to truce, his date. And that would be towards the end of the the next week, we should be ready for that. And in the week from the 11th, the sooner the better. We should run it life for them. And before that we should run it for ourselves, right?\\nPERSON6: Okay so. Sorry for not managing the the half an hour. But as you see, my kids are really getting hungry, and i still need to peel the potatoes, and. Thanks for joining, and will be in very close touch for for the demo in the coming days.\\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeoD-XjdDk8-"
      },
      "source": [
        "# OVERVIEW THE SECTIONED BLOCKS OF CONVERSATIONS FROM THE TRANSCRIPT ...\n",
        "\n",
        "id=1\n",
        "for i in tscs_preprocessed1[m_id]:\n",
        "  print('{} - {}'.format(id, i))\n",
        "  id+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQcS-1NADjYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2128d914-25f5-45b0-d913-b5c445d932ea"
      },
      "source": [
        "### THE BELOW 4 CELLS WOULD GIVE YOU 4 SUMMARIES VARYING IN LENGTH; ###\n",
        "### THIS WOULD NORMALLY AFFECT THE COVERAGE AND ADEQUACY OF THE SUMMARIES; ###\n",
        "### YOU CAN CHOOSE A SUITABLE SUMMARY FOR EVERY SINGLE TRANSCRIPT !!! ###\n",
        "\n",
        "s2_short, filename = gen_summary(tscs_preprocessed1)\n",
        "print(format_summary(s2_short[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  The Czech Republic government has lifted the rules.\n",
            "  People can go out even if they don't need to, but they have to wait until June for the free circulation of people.\n",
            " -PERSON5 is in Trento and PERSON13 is in Bolzano.\n",
            " -PERSON1, PERSON5, PERSON6, PERSON15, PERSON16 and PERSON8 will write a project management guide for Organizing Committee 6.\n",
            " -PERSON2 can has always believed in selforganization.\n",
            "  The internal reviews should be ready by mid June at the latest.\n",
            "  The project management guides are due in the end of June.\n",
            "  The test sets are due on the 8th of June PERSON6 and PERSON2 discusseded the layout of the PROJECT1 test set.\n",
            "  They want to have it populated and described by the August date so that they can submit as a deliver.\n",
            " -PERSON6, PERSON1 and PERSON9 discussed how to organize the test sets and create file lists.\n",
            " -PERSON6, PERSON1, PERSON9 and PERSON16 discusseded the evaluation of spoken language translation.\n",
            " -PERSON6 asks PERSON1 and PERSON3 to review the PROJECT1 test set.\n",
            "  The project was started when the EU still existed.\n",
            " -PERSON1, PERSON5, PERSON6, PERSON7, PERSON9 and PERSON16 worked on the project.\n",
            " -PERSON6, PERSON1, PERSON9 and PERSON2 discusseded whether retranslating is a priority for the project.\n",
            " -PERSON1, PERSON16, PERSON6, PERSON13 and PERSON4 discusseded the organization of the demo and how to present it to the partners.\n",
            " -PERSON1, PERSON6, PERSON9 and PERSON4 will have a demo next week.\n",
            "  The demo should be in both English and French.\n",
            "  They will test the demo in German.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UncQ_DOw5LGO"
      },
      "source": [
        "s2_avg, filename = gen_summary(tscs_preprocessed2)\n",
        "print(format_summary(s2_avg[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fxfDgkJ0PKN"
      },
      "source": [
        "s2_long, filename = gen_summary(tscs_preprocessed3)\n",
        "print(format_summary(s2_long[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppzUCF_iLrJB",
        "outputId": "359da5fa-10cf-4348-ce1b-f2e88b15c12e"
      },
      "source": [
        "print(attendees)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['PERSON5', 'PERSON9', 'PERSON1', 'PERSON6', 'PERSON16', 'PERSON15', 'PERSON13']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfHV5eAhRCOa"
      },
      "source": [
        "# SAVING THE MINUTE ...\n",
        "\n",
        "import datetime\n",
        "outfile = open('/content/drive/MyDrive/AutoMin-2021/outputs/new/{}.txt'.format(filename[0]), 'w')\n",
        "tday = datetime.date.today()\n",
        "formatted_summary = format_summary(s2_longer[0])\n",
        "att = ', '.join(attendees[0])\n",
        "outfile.write('DATE : {}\\nATTENDEES : {}\\n\\n\\nSUMMARY-\\n{}\\n\\n\\nMinuted by: Team ABC'.format(tday, att, formatted_summary))\n",
        "outfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A46U0TK9hzLn"
      },
      "source": [
        "### A FORMAT SUMMARY FUNCTION, WITHOUT PRONOUN INSERTION ###\n",
        "\n",
        "def format_summary1(s2):\n",
        "\n",
        "  s3 = ''.join(s2) #s2[0]\n",
        "\n",
        "  s3 = s3.split('.')\n",
        "  summ = ['']\n",
        "  id=0\n",
        "  \n",
        "  summ1 = []\n",
        "  for i in s3:\n",
        "    #stripping the spaces\n",
        "    i = i.replace('  ', ' ')\n",
        "    if len(i) == 1:\n",
        "      continue\n",
        "    if i[0]==' ' and i[1].isalpha():\n",
        "      i = stripp(i)\n",
        "    if type(i) == type(None):\n",
        "      continue\n",
        "    if i[0] == ' ':\n",
        "      continue\n",
        "    i = preprocess(i)\n",
        "    check = re.sub(r\"[^a-zA-Z0-9]+\", ' ', i)\n",
        "    check = ''.join(i for i in check if not i.isdigit())\n",
        "    check = check.replace('  ', ' ')\n",
        "    check = check.split(' ')\n",
        "    if len(check)<=6:\n",
        "      continue\n",
        "\n",
        "    #formatting\n",
        "    if i[0] == 'P' and i[1] == 'E':\n",
        "      id+=1\n",
        "      summ.append('')\n",
        "      summ[id] = summ[id] + ' -' + i + '.'\n",
        "    # elif i[0] in ['M','T','O','A'] and (i[1].isalpha()==False):\n",
        "    #   id+=1\n",
        "    #   summ.append('')\n",
        "    #   summ[id] = summ[id] + ' -' + i + '.'\n",
        "    # elif i[0]=='M' and i[1]=='U':\n",
        "    #   id+=1\n",
        "    #   summ.append('')\n",
        "    #   summ[id] = summ[id] + ' -' + i + '.'\n",
        "    else:\n",
        "      summ[id] = summ[id] + '\\n  ' + i + '.'\n",
        "\n",
        "  if '' in summ:\n",
        "    summ.remove('')\n",
        "  summ = '\\n'.join(summ)\n",
        "  return summ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCrO6YctzGdx"
      },
      "source": [
        "# If we want to further shorten the obtained summary...\n",
        "This method sacrifices gramaticality and readbility, in order to achieve compactness, by using NLTK stopword reduction over a general BART Summarization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIDuEJ4xLMF1"
      },
      "source": [
        "# RUN THE CELLS BELOW AND USE THIS FUNCTION INSTEAD OF THE 'format_summary()' version ...\n",
        "\n",
        "def format_summary_short(s2):\n",
        "  s3 = ''.join(s2) #s2[0]\n",
        "  s3 = s3.split('.')\n",
        "  summ = ['']\n",
        "  id=0\n",
        "  for i in s3:\n",
        "\n",
        "    #stripping the spaces\n",
        "    i = i.replace('  ', ' ')\n",
        "    if len(i) == 1:\n",
        "      continue\n",
        "    if i[0]==' ' and i[1].isalpha():\n",
        "      i = stripp(i)\n",
        "    if i[0] == ' ':\n",
        "      continue\n",
        "    check = re.sub(r\"[^a-zA-Z0-9]+\", ' ', i)\n",
        "    check = ''.join(i for i in check if not i.isdigit())\n",
        "    check = check.replace('  ', ' ')\n",
        "    check = check.split(' ')\n",
        "    if len(check)<=6:\n",
        "      continue\n",
        "\n",
        "    #formatting\n",
        "    if i[0] == 'P':\n",
        "      id+=1\n",
        "      summ.append('')\n",
        "      i = shorten(i)\n",
        "      i = replacee(i)\n",
        "      summ[id] = summ[id] + ' -' + i + '.'\n",
        "    else:\n",
        "      i = shorten(i)\n",
        "      i = replacee(i)\n",
        "      summ[id] = summ[id] + '\\n  ' + i + '.'\n",
        "\n",
        "  if '' in summ:\n",
        "    summ.remove('')\n",
        "  summ = '\\n'.join(summ)\n",
        "  return summ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWS4uhFM5mFl",
        "outputId": "37f69110-46a6-44e5-a285-d6366c2bd776"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEnIRKFx0ucc"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "def shorten(example_sent):\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  stop_words.remove('to')\n",
        "  stop_words.remove('of')\n",
        "  stop_words.remove('from')\n",
        "  stop_words.remove('as')\n",
        "  stop_words.remove('has')\n",
        "  stop_words.remove('do')\n",
        "  stop_words.remove('not')\n",
        "  #stop_words.remove('be')\n",
        "  stop_words.remove('on')\n",
        "  stop_words.remove('in')\n",
        "  stop_words.remove('if')\n",
        "  stop_words.remove('is')\n",
        "  stop_words.remove('it')\n",
        "  stop_words.remove('for')\n",
        "  stop_words.remove('with')\n",
        "  stop_words.remove('he')\n",
        "  stop_words.remove('can')\n",
        "  stop_words.remove('does')\n",
        "  stop_words.remove('between')\n",
        "  stop_words.add('They')\n",
        "  stop_words.add('which')\n",
        "  stop_words.add('On')\n",
        "  stop_words.add('It')\n",
        "  stop_words.add('The')\n",
        "  stop_words.remove('over')\n",
        "  stop_words.remove('until')\n",
        "  stop_words.remove('after') \n",
        "  stop_words.add('He')\n",
        "  stop_words.remove('when')\n",
        "  stop_words.remove('have')\n",
        "  stop_words.remove('them')\n",
        "  stop_words.remove('into')\n",
        "  stop_words.remove('by')\n",
        "  stop_words.remove('and')\n",
        "  stop_words.remove('will')\n",
        "  stop_words.remove('what')\n",
        "  stop_words.add('manually')\n",
        "  stop_words.remove('him')\n",
        "    \n",
        "  word_tokens = word_tokenize(example_sent) \n",
        "    \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "    \n",
        "  filtered_sentence = [] \n",
        "    \n",
        "  for w in word_tokens: \n",
        "      if w not in stop_words: \n",
        "          filtered_sentence.append(w) \n",
        "    \n",
        "  return ' '.join(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFE8hI97DR4x"
      },
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model=model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsAsGPTvD10v"
      },
      "source": [
        "summary = ''\n",
        "for s in s2[0]:\n",
        "  preprocess_text = s.strip().replace(\"\\n\",\"\")\n",
        "  inputs = tokenizer(preprocess_text, return_tensors='pt').to(device)\n",
        "  summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=512)\n",
        "  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "  summary = summary + output + ' '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19eC2c88Vr47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692e29e0-fb9c-4adc-fd63-ec3d233e3133"
      },
      "source": [
        "s2_shorter, filename = gen_summary(tscs_preprocessed4)\n",
        "print(format_summary_short(s2_shorter[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-PERSON14 is preparing data for the audible cell ties data preparation.\n",
            " PERSON5 is watching a lecture on PROJECT2 ORGANIZATION4 live streaming.\n",
            " .\n",
            "-PERSON5 needs to get the input files for the evaluation.\n",
            " The most important part is Antrecorp.\n",
            " PERSON5 and PERSON14 are on a conference call.\n",
            " They need to change the language or nation names in the released files.\n",
            " The most up to date OST versions are in the repositor.\n",
            " PERSON5 needs to create the support director for the files he's processing.\n",
            " He also needs to prepare the versioned files for the subtitles for the caraoke and the OST file for the preview.\n",
            " .\n",
            "-PERSON14 wants PERSON1 to synchronize with PERSON5 on the on the audible SLT test set and depth set forced alignment files.\n",
            " He wants them to run the alignments and produce OST files to OSTP plus ASS files.\n",
            " PERSON1's priorities are the SLT evaluation, the clean up of the stable and preparing the files that PERSON5 will digest.\n",
            " PERSON5 is working on a caraoke as timeview.\n",
            " The deadline is 25th, so he doesn't have to do anything with audible SLT and he can write the paper.\n",
            " .\n",
            "-PERSON9 is working on 3 languages Polish, German and Russian to collect test data for preparing WSLT.\n",
            " PERSON14 asked him to process Antrecorp test.\n",
            " PERSON5 will create a Python script for it.\n",
            " .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJh02iTH2tGS"
      },
      "source": [
        "# TextRank Scipt for ranking sentences...\n",
        "This method uses GloVe Embeddings to calculate similarity score with the help of cosine similairty, and ranks individual sentences with the help of the PageRank Algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBJaRqQAEHKV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt') # one time execution\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwK8kem-33KA"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO2wMYWo33yP"
      },
      "source": [
        "# ENTER THE MINUTE_ID\n",
        "min_id = 'minutes_en_test_001'\n",
        "\n",
        "import os\n",
        "path = '/content/drive/MyDrive/AutoMin-2021/outputs/new'\n",
        "os.chdir(path)\n",
        "summaries = []\n",
        "# for file1 in sorted(os.listdir()):\n",
        "sumfile = open(path+'/'+min_id+'.txt', 'r')\n",
        "summ = sumfile.readlines()\n",
        "summ = summ[5:-3]\n",
        "text = ''\n",
        "for line in summ:\n",
        "    line = line.replace(' -', '')\n",
        "    line = line.replace('  ', '')\n",
        "    line = line.replace('\\n', '')\n",
        "    text = text + line + ' '\n",
        "summaries.append(text)\n",
        "#break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtmWVTeH36yZ"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = []\n",
        "for s in summaries:\n",
        "  sentences.append(sent_tokenize(s))\n",
        "\n",
        "sentences = [y for x in sentences for y in x] # flatten list\n",
        "print('Total no. of sentences: ', len(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zKjmVRt4Gw_"
      },
      "source": [
        "# EXTRACT WORD VECTORS\n",
        "\n",
        "word_embeddings = {}\n",
        "f = open('/content/glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()\n",
        "\n",
        "\n",
        "# REMOVE PUNCTUATIONS, NUMBERS AND SPECIAL CHARACTERS\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "\n",
        "# MAKE ALPHABETS TO LOWERCASE\n",
        "clean_sentences = [s.lower() for s in clean_sentences]\n",
        "\n",
        "\n",
        "# REMOVE STOPWORDS\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
        "\n",
        "\n",
        "# EXTRACT SENTENCE VECTORS\n",
        "\n",
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors.append(v)\n",
        "\n",
        "\n",
        "# INITIALIZE A SIMILARITY MATRIX\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
        "\n",
        "\n",
        "# PAGERANK SCORING\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)\n",
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Er-FCrg4NFZ"
      },
      "source": [
        "# REVIEW THE RANKINGS BEFORE ELIMINATING IRRELEVANT INFO FROM THE SUMMARY\n",
        "\n",
        "for i in range(len(ranked_sentences)):\n",
        "  print(ranked_sentences[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmP_iRje4jef"
      },
      "source": [
        "# ENTER THE PERCENTAGE OF SENTENCES THAT SEEM UNIFORMATIONAL,  THIS NUMBER IS USUALLY AROUND ~15% FOR THE MINUTES BELONGING TO A LENGTHY TRANSCRIPT\n",
        "\n",
        "rem_perc = 0.15\n",
        "\n",
        "import math\n",
        "remove_count = math.ceil(len(sentences)*rem_perc)\n",
        "print('No. of sentences removed: ', remove_count)\n",
        "\n",
        "print('\\n\\nReduced Summary(jumbled): \\n')\n",
        "for i in range(len(ranked_sentences)-remove_count):\n",
        "  print(ranked_sentences[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}