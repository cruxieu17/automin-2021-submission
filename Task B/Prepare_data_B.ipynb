{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "TASKB.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vflZe0BU0DVZ",
        "m1dRaldhrGQa",
        "1dOWCyokbS07",
        "m7usmhcQ9_GO",
        "G8k7B0sxl9A0",
        "tYTkSWCGl5XC",
        "61SFEpI660d0",
        "QUFLiC1grr4N",
        "4e-aZwhpfNKO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "len(os.listdir('/content/drive/MyDrive/automin-2021-confindential-data-main/task-B-en/train'))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVqXcqpwXpxj",
        "outputId": "1ba3f2aa-cf06-47b1-a7ef-a4c4b1ef2192"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZBUBO9RiXjN",
        "outputId": "f8f88a81-c4e6-4a39-9890-b86ee64fbc1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install rouge"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9XvPag9rJ0Z",
        "outputId": "6b2d795e-1115-4b0d-b5af-6bcba629c6ae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import re \r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\r\n",
        " \r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        " \r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "import spacy\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "import torch\r\n",
        "\r\n",
        "# metrics\r\n",
        "from difflib import SequenceMatcher\r\n",
        "from rouge import Rouge"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "id": "pzLSoqUSiaR2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Remove boundaries [PERSONx], (PERSONy) etc. to PERSONx, PERSONy\r\n",
        "import re\r\n",
        " \r\n",
        "def rem_boundries(wordList):\r\n",
        " \r\n",
        "  processed_list = []\r\n",
        " \r\n",
        "  reg_Ex = [r\"\\(\", r\"\\)\", r\"\\[\", r\"]\"]\r\n",
        " \r\n",
        "  for word in wordList:\r\n",
        "    for exp in reg_Ex:\r\n",
        "      word = re.sub(exp, '', word)\r\n",
        "    processed_list.append(word)\r\n",
        " \r\n",
        "  return processed_list"
      ],
      "outputs": [],
      "metadata": {
        "id": "na2JV5n6LUSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def brac_match(reg_ex, TextList):\r\n",
        "  \r\n",
        "  processed_list = []\r\n",
        " \r\n",
        "  for line in TextList:\r\n",
        "    for exp in reg_ex:\r\n",
        "      result = re.findall(exp, line)\r\n",
        "      if len(result) > 0:\r\n",
        "        for word in result:\r\n",
        "          if word not in processed_list:\r\n",
        "            processed_list.append(word)\r\n",
        " \r\n",
        "  return processed_list"
      ],
      "outputs": [],
      "metadata": {
        "id": "omyP-hfnNKQG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_brac_cnt(Textlist):\r\n",
        "  \r\n",
        "  Reg_Ex = [r\"\\(PERSON\\d+\\)\", r\"\\[PERSON\\d+\\]\"] \r\n",
        "  Reg_Ex_ = [r\"\\[ORGANIZATION\\d+\\]\", r\"\\[PROJECT\\d+\\]\"]\r\n",
        "  \r\n",
        "  List_A = brac_match(Reg_Ex, Textlist)\r\n",
        "  List_A =  rem_boundries(List_A)\r\n",
        " \r\n",
        "  List_B = brac_match(Reg_Ex_, Textlist)\r\n",
        "  List_B =  rem_boundries(List_B)\r\n",
        " \r\n",
        "  if len(List_B) > len(List_A):\r\n",
        "    return List_B\r\n",
        " \r\n",
        "  return list(set(List_A))\r\n",
        " \r\n",
        "# x, y = get_brac_cnt(minutes), get_brac_cnt(transcripts)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Mo07m1tHDiY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def chk_person_match(meet_br_cnt, trans_br_cnt):\r\n",
        " \r\n",
        "  similar_cnt = []\r\n",
        " \r\n",
        "  for word in meet_br_cnt:\r\n",
        "    if word in trans_br_cnt:\r\n",
        "      similar_cnt.append(word)\r\n",
        " \r\n",
        "  return similar_cnt\r\n",
        " \r\n",
        "# c = chk_person_match(x, y)\r\n",
        "# c, len(c), len(x)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9yFu151GLdjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def remove_cntsqbr(text_list):\r\n",
        "  \r\n",
        "  new_list = []\r\n",
        " \r\n",
        "  for word in text_list:\r\n",
        "    new_list.append(re.sub(r\"( +)\\[(.*?)\\]\", '', word))\r\n",
        " \r\n",
        "  return new_list"
      ],
      "outputs": [],
      "metadata": {
        "id": "ooUoF3Zwjyr5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# toknize text\r\n",
        "def remove_punctuation(text):\r\n",
        " \r\n",
        "  tokenizer = RegexpTokenizer(r\"\\w+\")\r\n",
        " \r\n",
        "  return ' '.join(tokenizer.tokenize(text))"
      ],
      "outputs": [],
      "metadata": {
        "id": "lYnoRqm2z4h4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# remove stopwords from the text\r\n",
        "def remove_stopwords(stop_words, text):\r\n",
        " \r\n",
        "  wordsList = word_tokenize(text)\r\n",
        " \r\n",
        "  wordsList = [w for w in wordsList if w not in stop_words]\r\n",
        " \r\n",
        "  return wordsList"
      ],
      "outputs": [],
      "metadata": {
        "id": "TeJP0hd0zUd2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# add positional tag to word tokens\r\n",
        "def add_postag(word_tokens):\r\n",
        "  \r\n",
        "  tagged = nltk.pos_tag(word_tokens)\r\n",
        " \r\n",
        "  return tagged"
      ],
      "outputs": [],
      "metadata": {
        "id": "d0SmahxG0Ia4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def add_postag_spacy(text):\r\n",
        "\r\n",
        "  tagged = []\r\n",
        "\r\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\r\n",
        "\r\n",
        "  word_tokens = nlp(text)\r\n",
        "\r\n",
        "  for token in word_tokens:\r\n",
        "    tagged.append((token, token.pos_))\r\n",
        "\r\n",
        "  return tagged"
      ],
      "outputs": [],
      "metadata": {
        "id": "atDGIIuoQDtQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_NOUNS_spacy(tagged_tokens):\r\n",
        "\r\n",
        "  Extracted_Nouns = []\r\n",
        "\r\n",
        "  Extracted_Nouns = [word for word, tag in tagged_tokens if tag == 'NOUN']\r\n",
        "\r\n",
        "  return Extracted_Nouns\r\n",
        "\r\n",
        "# get_NOUNS_spacy(tagged)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gJkFqQOpRPug"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def aggregate_nouns_spacy(textList):\r\n",
        "\r\n",
        "  tot_nouns = []\r\n",
        "\r\n",
        "  for text in textList:\r\n",
        "    text = remove_punctuation(text)\r\n",
        "    tag_list = add_postag_spacy(text)\r\n",
        "    nouns = get_NOUNS_spacy(tag_list)\r\n",
        "    tot_nouns.extend(nouns)\r\n",
        "\r\n",
        "  tot_nouns = [word for word in tot_nouns if len(word) > 6]\r\n",
        "\r\n",
        "  return tot_nouns"
      ],
      "outputs": [],
      "metadata": {
        "id": "SGwfqud1wn8u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_NNS(tagged_tokens):\r\n",
        " \r\n",
        "  Extracted_Nouns = []\r\n",
        "    \r\n",
        "  # Extracted_Nouns = [word for word, tag in tagged_tokens if tag == 'NN' or tag == 'NNS']\r\n",
        "  Extracted_Nouns = [word for word, tag in tagged_tokens if tag == 'NN' or tag == 'JJ']\r\n",
        "  # Extracted_Nouns = [word for word, tag in tagged_tokens if tag == 'NNP' or tag == \"NN\"]\r\n",
        " \r\n",
        "  return Extracted_Nouns"
      ],
      "outputs": [],
      "metadata": {
        "id": "x_aH1q0H7P3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def aggregate_nouns(stop_words, textList):\r\n",
        " \r\n",
        "  tot_nouns = []\r\n",
        " \r\n",
        "  for text in textList:\r\n",
        "    text = remove_punctuation(text)\r\n",
        "    word_tokens = remove_stopwords(stop_words, text)\r\n",
        "    tag_list = add_postag(word_tokens)\r\n",
        "    nouns = get_NNS(tag_list)\r\n",
        "    tot_nouns.extend(nouns)\r\n",
        " \r\n",
        "  tot_nouns = [word for word in tot_nouns if len(word) > 5]\r\n",
        " \r\n",
        "  return tot_nouns\r\n",
        " \r\n",
        "# x = aggregate_nouns(stop_words, transcripts)\r\n",
        "# z = aggregate_nouns(stop_words, minutes)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BRph-PwX8-wo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def Counter_Transcripts(transcript):\r\n",
        " \r\n",
        "  return Counter(transcript)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UuMEX-XZP2Yr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def restrict_counter(Counter_Transcript):\r\n",
        " \r\n",
        "    prepro_counter = dict()\r\n",
        " \r\n",
        "    for key, value in Counter_Transcript.items():\r\n",
        " \r\n",
        "      # if value < 10:\r\n",
        "      #   prepro_counter[key] = value\r\n",
        " \r\n",
        "      if value > 10:\r\n",
        "        prepro_counter[key] = 10\r\n",
        "      else:\r\n",
        "        prepro_counter[key] = value\r\n",
        " \r\n",
        " \r\n",
        "    return prepro_counter"
      ],
      "outputs": [],
      "metadata": {
        "id": "XOyOyHMfTuT_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_similar_nouns(minute_nouns, transcript_nouns):\r\n",
        " \r\n",
        "  similar_nouns = [w for w in minute_nouns if w in transcript_nouns]\r\n",
        " \r\n",
        "  return similar_nouns"
      ],
      "outputs": [],
      "metadata": {
        "id": "AdPm_Wt6fil8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def meet_word_count(minute_nouns, transcript_nouns_Counter):\r\n",
        " \r\n",
        "  transcript_Counter = restrict_counter(transcript_nouns_Counter)\r\n",
        " \r\n",
        "  trans_nouns_set = list(transcript_Counter.keys())\r\n",
        "  trans_nouns_count = list(transcript_Counter.values())\r\n",
        " \r\n",
        "  count_p_index = []\r\n",
        " \r\n",
        "  for word in minute_nouns:\r\n",
        "    for idx, t_word in enumerate(trans_nouns_set):\r\n",
        "      if word == t_word:\r\n",
        "        count_p_index.append(trans_nouns_count[idx])\r\n",
        " \r\n",
        "  return count_p_index"
      ],
      "outputs": [],
      "metadata": {
        "id": "Sd6aoCF6mHtZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def calculate_Avg(minute_Count):\r\n",
        " \r\n",
        "  c_Tensor = torch.Tensor(minute_Count)\r\n",
        " \r\n",
        "  mean = torch.mean(c_Tensor)\r\n",
        " \r\n",
        "  return mean.item()"
      ],
      "outputs": [],
      "metadata": {
        "id": "-8EhwAEsY73V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def concat_nouns(minute_list, transcript_list):\r\n",
        " \r\n",
        "  Nounlist_A = list(set(minute_list))\r\n",
        " \r\n",
        "  trans_Counter = Counter_Transcripts(transcript_list)\r\n",
        "  Trans_dict = restrict_counter(trans_Counter)  \r\n",
        "  Nounlist_B = Trans_dict.keys()\r\n",
        " \r\n",
        "  minute_str = ' '.join(Nounlist_A)\r\n",
        "  transcript_str = ' '.join(Nounlist_B)\r\n",
        " \r\n",
        "  return minute_str, transcript_str"
      ],
      "outputs": [],
      "metadata": {
        "id": "ck1h7pUph2b4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def lowercase(word_list):\r\n",
        "\r\n",
        "  lower_list = []\r\n",
        "\r\n",
        "  for word in word_list:\r\n",
        "    lower_list.append(word.lower())\r\n",
        "\r\n",
        "  return lower_list\r\n",
        "\r\n",
        "def apply_stemming(word_list, flag=True):\r\n",
        "  \r\n",
        "  if flag:\r\n",
        "    word_list = list(set(word_list))\r\n",
        "\r\n",
        "  ps = PorterStemmer()  \r\n",
        "\r\n",
        "  stemmed_words = []\r\n",
        "\r\n",
        "  for word in word_list:\r\n",
        "    stemmed_words.append(ps.stem(word))\r\n",
        "\r\n",
        "  if flag:\r\n",
        "    stemmed_words = list(set(stemmed_words))\r\n",
        "\r\n",
        "  return stemmed_words"
      ],
      "outputs": [],
      "metadata": {
        "id": "SV5ihzbrnpGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def match_words(Minute_nouns, Transcript_nouns):\r\n",
        "\r\n",
        "  Minute_nouns = list(set(Minute_nouns))  \r\n",
        "  Transcript_nouns = list(set(Transcript_nouns))\r\n",
        "\r\n",
        "  Count = 0\r\n",
        "  for word in Minute_nouns:\r\n",
        "    if word in Transcript_nouns:\r\n",
        "      Count += 1"
      ],
      "outputs": [],
      "metadata": {
        "id": "fRAgbhrDpuc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "To_remove_list = ['something', 'meeting', 'transcript', 'arrival', 'system', 'suggestion', 'annotation']\r\n",
        "\r\n",
        "def remove_from(text_list):\r\n",
        "\r\n",
        "  text_list = list(set(lowercase(text_list)))\r\n",
        "\r\n",
        "  sample = []\r\n",
        "\r\n",
        "  for word in text_list:\r\n",
        "    if word not in To_remove_list:\r\n",
        "      sample.append(word)\r\n",
        "\r\n",
        "  return sample"
      ],
      "outputs": [],
      "metadata": {
        "id": "-7AHgKtBXnhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "#### Score 1\n",
        "----"
      ],
      "metadata": {
        "id": "MJZv54zFq80E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Score 1\r\n",
        "# aggregate methods to generate score\r\n",
        "\r\n",
        "def cnt_aggregator(minute, transcript, threshold = 0.8):\r\n",
        "\r\n",
        "  minute_cnt = get_brac_cnt(minute)\r\n",
        "  if len(minute_cnt) == 0:\r\n",
        "    return 0\r\n",
        "  else:\r\n",
        "    trans_cnt = get_brac_cnt(transcript)\r\n",
        "    match = chk_person_match(minute_cnt, trans_cnt)\r\n",
        "    m_len = len(match)\r\n",
        "    min_len = len(minute_cnt)\r\n",
        "    score = len(match)/len(minute_cnt)\r\n",
        "    if score > threshold:\r\n",
        "      return 1\r\n",
        "    else: \r\n",
        "      return 0"
      ],
      "outputs": [],
      "metadata": {
        "id": "3lILE8n9YEtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "#### Score2\n",
        "----"
      ],
      "metadata": {
        "id": "FszxcTdQovUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Score 2\r\n",
        "# get the nouns list for both minutes and Transcript\r\n",
        "# apply aggregate function\r\n",
        "\r\n",
        "def overall_aggregator(minute, transcript, stopwords):\r\n",
        "\r\n",
        "  # Remove the content within square brackets \r\n",
        "  minute = remove_cntsqbr(minute)\r\n",
        "  transcript = remove_cntsqbr(transcript)  \r\n",
        "\r\n",
        "  # Remove punctutations, stop_words -> Add pos_tag -> Get NNS || NN\r\n",
        "  Minute_nouns = aggregate_nouns(stopwords, minute)\r\n",
        "  Transcript_nouns = aggregate_nouns(stopwords, transcript)\r\n",
        "\r\n",
        "  # Apply Counter to Transcripts\r\n",
        "  Transcript_nouns_Counter = Counter(Transcript_nouns)\r\n",
        "\r\n",
        "  # check the word count corresponding to similarity of minute nouns in transcript nouns\r\n",
        "  word_count = meet_word_count(Minute_nouns, Transcript_nouns_Counter)\r\n",
        "\r\n",
        "  # Calculat Average distinct word count of minutes nouns in transcript nouns\r\n",
        "  avg_value = calculate_Avg(word_count)\r\n",
        "\r\n",
        "  return avg_value\r\n",
        "\r\n",
        "# average = overall_aggregator(minutes, transcripts, stop_words)\r\n",
        "\r\n",
        "# overall_aggregator(minutes, transcripts, stop_words)"
      ],
      "outputs": [],
      "metadata": {
        "id": "8OTBkvhF8gFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "#### Score 3 - 7\n",
        "----"
      ],
      "metadata": {
        "id": "96ckIXDno-B1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# calculate cosine similarity \r\n",
        "def cosine_similarity(X,Y):\r\n",
        "  \r\n",
        "  X_list = word_tokenize(X) \r\n",
        "  Y_list = word_tokenize(Y)\r\n",
        "  sw = stopwords.words('english') \r\n",
        "  l1 =[];l2 =[]\r\n",
        "  X_set = {w for w in X_list if not w in sw} \r\n",
        "  Y_set = {w for w in Y_list if not w in sw}\r\n",
        "\r\n",
        "\r\n",
        "  rvector = X_set.union(Y_set) \r\n",
        "  for w in rvector:\r\n",
        "      if w in X_set: l1.append(1) # create a vector\r\n",
        "      else: l1.append(0)\r\n",
        "      if w in Y_set: l2.append(1)\r\n",
        "      else: l2.append(0)\r\n",
        "  c = 0\r\n",
        "\r\n",
        "\r\n",
        "  for i in range(len(rvector)):\r\n",
        "          c+= l1[i]*l2[i]\r\n",
        "  cosine = c / float((sum(l1)*sum(l2))**0.5)\r\n",
        "  return cosine"
      ],
      "outputs": [],
      "metadata": {
        "id": "uX3yx-9vj6Lf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Seqeunce matcher function\r\n",
        "def sequence_matcher(a, b):\r\n",
        "  return SequenceMatcher(None, a, b).ratio()"
      ],
      "outputs": [],
      "metadata": {
        "id": "mB9vILeVkfpy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Rouge scores rouge1 and rougel\r\n",
        "def Rouge_Score(hypothesis, reference):\r\n",
        "  rouge = Rouge()\r\n",
        "  scores = rouge.get_scores(hypothesis, reference)\r\n",
        "  \r\n",
        "  return scores"
      ],
      "outputs": [],
      "metadata": {
        "id": "CXOj1IVhk59C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# self defined res function\r\n",
        "def res(a,b):\r\n",
        "    return len(set(a.split(' ')) & set(b.split(' '))) / float(len(set(a.split(' ')) | set(b.split(' ')))) *100"
      ],
      "outputs": [],
      "metadata": {
        "id": "ol-C5ECSl2tH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# aggregate methods and calculate score 3 to 7\r\n",
        "\r\n",
        "def aggregate_concat(minutes, transcripts):\r\n",
        " \r\n",
        "  # Remove the content within square brackets \r\n",
        "  minute = remove_cntsqbr(minutes)\r\n",
        "  transcript = remove_cntsqbr(transcripts)  \r\n",
        " \r\n",
        "  # Remove punctutations, stop_words -> Add pos_tag -> Get NNS || NN\r\n",
        "  Minute_nouns = aggregate_nouns(stop_words, minute)\r\n",
        "  Transcript_nouns = aggregate_nouns(stop_words, transcript)\r\n",
        " \r\n",
        "  minute_str, transcript_str = concat_nouns(Minute_nouns, Transcript_nouns)\r\n",
        " \r\n",
        "  cos = cosine_similarity(minute_str, transcript_str)\r\n",
        "  r_1 = Rouge_Score(minute_str, transcript_str)[0]['rouge-1']['f']\r\n",
        "  r_l = Rouge_Score(minute_str, transcript_str)[0]['rouge-l']['f']\r\n",
        "  seq = sequence_matcher(minute_str, transcript_str)\r\n",
        "  reS = res(minute_str, transcript_str)\r\n",
        " \r\n",
        "  return cos, r_1, r_l, seq, reS"
      ],
      "outputs": [],
      "metadata": {
        "id": "CvPZkZQ8jczF"
      }
    }
  ]
}
